,id,type,action,actor_id,actor_login,repo_id,repo_name,org_id,org_login,created_at,issue_id,issue_number,issue_title,body,issue_author_id,issue_author_login,issue_assignee_id,issue_assignee_login,issue_assignees.login,issue_assignees.id,issue_comment_id,issue_comment_author_id,issue_comment_author_login,pull_merge_commit_sha,pull_merged_by_id,pull_merged_by_login,pull_requested_reviewer_id,pull_requested_reviewer_login,pull_base_ref,pull_head_repo_id,pull_head_repo_name,pull_head_ref,pull_review_id,pull_review_comment_id,pull_review_comment_path,pull_review_comment_author_id,pull_review_comment_author_login,push_id,push_head,push_commits.name,push_commits.email,push_commits.message,commit_comment_id,commit_comment_author_id,commit_comment_author_login,commit_comment_path,commit_comment_sha,release_id,release_tag_name,release_name,release_author_id,release_author_login,release_body,release_assets.name
0,20498547959,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-02-28 14:33:35,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
1,25916554550,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-15 17:51:19,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
2,25970539905,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-19 10:51:25,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
3,25970973820,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-19 11:11:24,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
4,26003338796,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-20 17:01:32,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
5,26019203484,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-21 11:05:45,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
6,26023939304,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-21 14:52:31,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
7,21770726392,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-13 08:48:48,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
8,22043847057,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-29 19:17:34,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
9,21363012694,CommitCommentEvent,added,1933698,s2hc-johan,791522,basho/riak_kv,176293,basho,2022-04-19 21:00:20,0,0,,"so the arity in this call changed, but don’t you have to change the function itself as well, or is it already in place?",0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],71654339,1933698,s2hc-johan,src/riak_kv_vnode.erl,f54ffa3c657087613b714b1d67e60d8df9c0a860,0,,,0,,,[]
10,21363046595,CommitCommentEvent,added,1933698,s2hc-johan,791522,basho/riak_kv,176293,basho,2022-04-19 21:02:36,0,0,,"Haven’t checked the full code flow, but why logging here and not call the function that would yield a read repair? Don’t we want that, I mean if it’s called earlier we don’t have to log once more and if it hasn’t been called we would like to so we get a read repair",0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],71654453,1933698,s2hc-johan,src/riak_kv_index_hashtree.erl,f54ffa3c657087613b714b1d67e60d8df9c0a860,0,,,0,,,[]
11,21363505644,CommitCommentEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-19 21:35:18,0,0,,"My concern here is about an endless cycle.  We get to expand_item/4 on every PUT, and that PUT might be triggered by a read_repair.  If the read_repair causes a tree change which crashes again at this point ... we'll keep looping round.  So we read_repair off the the fold, but not here so that we don't get stuck in a loop.",0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],71656357,1628897,martinsumner,src/riak_kv_index_hashtree.erl,f54ffa3c657087613b714b1d67e60d8df9c0a860,0,,,0,,,[]
12,21363519376,CommitCommentEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-19 21:36:21,0,0,,"Yes.  There is a change in rebar.config to pull a different version of kv_index_tictactree which supports the 3 arity version, as well as the old 2 arity one.",0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],71656431,1628897,martinsumner,src/riak_kv_vnode.erl,f54ffa3c657087613b714b1d67e60d8df9c0a860,0,,,0,,,[]
13,24397856453,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-10-04 21:05:30,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
14,24532527731,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-10-11 17:42:02,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
15,20112299625,CreateEvent,added,3169010,martincox,791522,basho/riak_kv,176293,basho,2022-02-07 16:06:38,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
16,20129115522,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-02-08 12:17:37,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
17,20132261329,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-02-08 14:50:42,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
18,25572007295,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-11-30 18:29:24,1468915668,1839,,"As I understand it convert_fold is only used normally via `riak_client:aae_fold/1`.  However this is broken too:

https://github.com/basho/riak_kv/blob/9ca718423ea2f37a653b933886ef1650a5e02de7/src/riak_client.erl#L905-L911

The `Q0` is used in the validity check, but not in the actual query (where `Query` is used).

Do you want to make this quick fix as part of this PR, and pass Q0 through to the query?

I have a riak_test ready.  This test extends `verify_aaefold_rangerepair` so that it works via riak_client when depending on the date conversion: https://github.com/basho/riak_test/blob/mas-1839-convertfold/tests/verify_aaefold_rangerepair.erl.

If you update the PR with the second fix, the test should pass",1107079,hmmr,0,,[],[],1332571109,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
19,23043223955,ForkEvent,added,38819652,LoisSotoLopez,791522,basho/riak_kv,176293,basho,2022-07-23 13:34:25,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
20,19876985918,ForkEvent,added,93924620,jimzhangex,791522,basho/riak_kv,176293,basho,2022-01-24 01:12:36,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
21,20644899902,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-08 20:03:32,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
22,20666951575,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-09 20:24:27,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
23,20716947710,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-12 19:57:18,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
24,20760477810,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-15 15:45:11,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
25,22082633481,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-31 18:57:21,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
26,22083216917,DeleteEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-31 19:32:44,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
27,25399858306,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-11-22 11:14:53,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
28,25438197909,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-11-23 19:30:29,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
29,25698528527,DeleteEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-06 18:18:12,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
30,25971651960,DeleteEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-19 11:45:03,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
31,26019661412,DeleteEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-21 11:29:06,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
32,21247485041,CreateEvent,added,3169010,martincox,791522,basho/riak_kv,176293,basho,2022-04-12 13:15:17,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
33,21335144272,CreateEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-18 13:44:50,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
34,21756264151,DeleteEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 14:28:54,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
35,21756883422,DeleteEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 14:55:41,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
36,21759974521,DeleteEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 17:39:13,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
37,21761591962,DeleteEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 19:21:16,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
38,26086252228,WatchEvent,started,5274430,rclijia,791522,basho/riak_kv,176293,basho,2022-12-26 03:30:08,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
39,23264916771,WatchEvent,started,55653825,mox692,791522,basho/riak_kv,176293,basho,2022-08-04 16:26:12,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
40,23418319331,WatchEvent,started,67239726,elsvc,791522,basho/riak_kv,176293,basho,2022-08-13 09:42:11,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
41,23670384620,WatchEvent,started,16653067,maitysubhasis,791522,basho/riak_kv,176293,basho,2022-08-27 08:46:13,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
42,24669381495,DeleteEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-10-18 12:46:42,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
43,20695303032,ForkEvent,added,1087071,lihuibng,791522,basho/riak_kv,176293,basho,2022-03-11 07:29:38,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
44,20869992929,ForkEvent,added,37914580,jpantao,791522,basho/riak_kv,176293,basho,2022-03-22 11:00:23,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
45,22144944962,WatchEvent,started,42808204,Nonnnnnnnnn,791522,basho/riak_kv,176293,basho,2022-06-03 10:57:40,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
46,22148458877,WatchEvent,started,16603080,Fov6363,791522,basho/riak_kv,176293,basho,2022-06-03 14:23:06,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
47,22191278285,WatchEvent,started,39606633,kvoli,791522,basho/riak_kv,176293,basho,2022-06-06 23:27:09,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
48,20412340164,IssueCommentEvent,created,1107079,hmmr,791522,basho/riak_kv,176293,basho,2022-02-23 12:47:45,1075973703,1809,,"The proposed change implements a disk-backed buffer for delete and reap request queues (as opposed to in-memory queues). It is a mitigation against said queues growing and consuming too much memory, and potentially causing OOM conditions on a node, observed when, for example, a node re-joins its cluster after substantial amount of data has been modified/deleted in the cluster, causing massive AAE activity with which the joining node is unable to cope.

I can see all pieces in the flow and I find eunit tests with good coverage, which I can confirm are passing.

One concern that I have is about stale disk_log files: I agree that [""wiping of wrong data""](https://github.com/basho/riak_kv/pull/1809/commits/fee56e5931d67090b8b4407982569b83ac91e3a8) is a valid argument to the contrary; but something still needs to be done to prevent stale files from appearing. What if `riak_kv.eraser_dataroot` and `riak_kv.reaper_dataroot` are placed in a temp (uuid-based) dir, specifically created at node startup, under some `riak_kv.disklog_queue_dir`? Users can then be able to distinguish between the dir currently in use from dirs left behind, and delete them without the need to stop the node.

Other than that, a couple of nits: no need to [++ strings](https://github.com/basho/riak_kv/pull/1809/files#diff-24efacea5e7b7d6be1d68972e4c37eb252e661437b2e6eb25c4cb193e58df103R126), and [lots](https://github.com/basho/riak_kv/pull/1809/files#diff-13190cb1d6b02cf95feecfe576f9c30f09e066b6e1ff102c409d463ba9040d02R96) of [trailing](https://github.com/basho/riak_kv/pull/1809/files#diff-0e69c47b07842327ba0cecd5c2f63c22852b8a0767fd99a3dc8c12b8b1f8d57fR83) [whitespace](https://github.com/basho/riak_kv/pull/1809/files#diff-24efacea5e7b7d6be1d68972e4c37eb252e661437b2e6eb25c4cb193e58df103R151). Bonus points for using a solution from stdlib (disk_log) instead of reinventing it!",1628897,martinsumner,0,,[],[],1048747552,1107079,hmmr,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
49,20432343611,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-02-24 11:56:20,1075973703,1809,,"@hmmr With regards to the use of '++', is this a suggestion to use iolists instead, with the improved efficiency?",1628897,martinsumner,0,,[],[],1049784953,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
50,20452415812,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-02-25 12:32:27,1051942480,1804,,"The implementation of this will use a separate process `riak_kv_replrtq_peer`.  This process will always be started, but will only actively poll if peer_discovery is enabled.

When triggered the peer discovery process will:

- View the Queue/Peer configuration passed in at startup `app_helper:get_env(riak_kv, replrtq_sinkpeers, """")`.  
- Call the `riak_kv_replrtq_snk` for a {list of current peers, worker_count, per_peer_limit} for each of the configured queues.
- Call each configured peer for that queue, and discover the active peers within the cluster (this process should be managed by the `riak_kv_replrtq_peer` process on the source via a new API call).
- Find the union of all active peers discovered for that queue.
- Compare the new set of peers with the previous set of current peers returned from the riak_kv_replrtq_snk - if the sets of peers match, then no action should be taken.
- If the peers differ, use the `remove_snkqueue/1` & `add_snkqueue/4` to update the configuration of the `riak_kv_replrtq_snk`.

The trigger will be a slow poll, perhaps once per hour by default (randomised to reduce risk of coordination).  If a peer temporarily goes down/up, the existing `riak_kv_replrtq_snk`  should handle this as normal.  

There needs to be an additional console command:

- reset_all_peers(WorkerCount, PerPeerLimit): visit each active peer in this cluster and trigger (one at a time) the peer discovery process for each active node in the cluster.  If WorkerCount or PerPeerLimit are passed, then override these settings (even if the peers have not changed).  If peer discovery is not enabled, reset_all_peers will only change the WorkerCount and PerPeerLimit if it differs from that returned by the riak_kv_replrtq_snk - the command will not discover where discovery is not enabled.

The only change required to `riak_kv_replrtq_snk` is the capability to extract the current peer information.  If peer discovery is not enabled (which will be the default), this will behave exactly as before - so the new behaviour is backwards compatible.  Any issues with peer discovery, and it can be disabled and previous configuration be relied upon.

If peer discovery is enabled, the behaviour of `riak_kv_replrtq_snk` is only interrupted should there be a need for configuration change.  From a configuration perspective, all known peers can continue to be configured, without penalty.  The `riak_kv_replrtq_peer` process should handle exceptions when contacting a peer, the unavailability of a configured peer should not crash the process.

The new peer discovery process will only work should the capability exist in both clusters.  Replication will not work as expected if peer discovery is enabled, and all nodes in all clusters are at the minimum required version.  Peer discovery must only be enabled, once the administrator has confirmed that it is supported across the domain - this will not fail gracefully.
",1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],1050814831,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
51,25569158393,WatchEvent,started,11502169,zavier,791522,basho/riak_kv,176293,basho,2022-11-30 16:20:58,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
52,23776005402,WatchEvent,started,10686206,nir-zilberman,791522,basho/riak_kv,176293,basho,2022-09-01 21:01:16,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
53,23906582853,WatchEvent,started,108923840,christian-arias-scaleaq,791522,basho/riak_kv,176293,basho,2022-09-09 01:53:47,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
54,21247488759,DeleteEvent,added,3169010,martincox,791522,basho/riak_kv,176293,basho,2022-04-12 13:15:27,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
55,24269987802,WatchEvent,started,29781322,padma0,791522,basho/riak_kv,176293,basho,2022-09-28 09:30:30,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
56,24310844731,WatchEvent,started,36593361,sre990,791522,basho/riak_kv,176293,basho,2022-09-29 22:11:41,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
57,24318583310,WatchEvent,started,38412458,auula,791522,basho/riak_kv,176293,basho,2022-09-30 08:43:02,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
58,25350286563,ForkEvent,added,603610,efcasado,791522,basho/riak_kv,176293,basho,2022-11-19 17:37:08,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
59,21597536430,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-03 16:01:46,1224275889,1826,,"Firstly of note, there are some default changes to standard erl flags in riak:

[load compaction](https://www.erlang.org/doc/man/erl.html#+scl)
- this is disabled by default in Riak 3.0.9

[utilisation balancing](https://www.erlang.org/doc/man/erl.html#+sub)
- this is enabled by default in Riak 3.0.9

[force wake-up](https://www.erlang.org/doc/man/erl.html#+sfwi)
- this is set to 500ms in Riak 3.0.9

[Async threads](https://www.erlang.org/doc/man/erl.html#emulator-flags)
- this is set to 64 in Riak 3.0.9

The first three items are changed form default, I believe, due to issues with scheduler collapse in Riak on earlier OTP versions.  When using leveled backend, for Riak 3.0.10, it makes sense to revert these on-default settings as there almost all I/O is not using inbuilt dirty I/O NIFs.

Likewise the async threads are now redundant when using leveled.

Caution is required if leveldb or bitcask backends are used as they do not necessarily use ""Dirty"" schedulers.  Likewise if standard (non-tictac) kv_index_hashtree-based AAE is used (which will use a leveldb backend).

When testing with erlang defaults, not basho defaults, and a leveled/tictac_aae system - our standard 24 hour test saw a 1.2 % throughput improvement.
",1628897,martinsumner,0,,[],[],1116267593,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
60,21597777827,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-03 16:15:00,1224275889,1826,,"By default Riak will start with one ""standard"" scheduler per vCPU, one ""dirty"" CPU scheduler per vCPU,  64 async threads and ten ""dirty IO"" schedulers.

This means there are many more schedulers than there are vCPUs.

There are two different changes to this which have bene initially tried:

1- Use a lower percentage of online schedulers (100%/75% for standard schedulers, 50%/25% for dirty CPU schedulers, 4 async threads).  This reduces the ration of online schedulers to vCPUs.
2- Use the [default bind type](https://www.erlang.org/doc/man/erl.html#+sbt) keep the same amount of schedulers but bind them directly to the vCPUs.

The comparative throughput between tests with these different configurations can be seen here:

<img width=""1579"" alt=""ThroughputComparison_24hr_LinearScale"" src=""https://user-images.githubusercontent.com/1628897/166491816-c3a54102-cce0-44ff-b8ea-04eb897f484f.png"">

On a log scale to show the delta at the end of the test (when the test is a more realistic representation of performance in a loaded cluster): 

<img width=""1574"" alt=""ThroughputComparison_24hr_LogScale"" src=""https://user-images.githubusercontent.com/1628897/166492035-1dbfb205-15be-4207-850e-e0aded8c2149.png"">

The test is a 24-hour test which is split into two phases.  The first phase has a comparatively high proportion of PUTs, the second has a comparatively high proportion of GETs.  The number of deletes and 2i queries remains constant.

The test is conducted on an 8-node cluster, with each node having 96GB RAM, a series of spinning disks in RAID 10, 2 hex core CPUs (with each core showing as 2 vCPUs), and flash-backed write-cache.

The x-axis of the chart shows the accumulated updates at this point of the test.  The y-axis the transactions per seconds (GET, PUT, 2i query, DELETE combined).  Each point represents a throughput reading measure over a 10s period.  Measurements are taken every 10s in the test.

At the 250M update point, the relative throughput improvement when compared to the same Riak test with basho default settings are:

- reduced scheduler count - 22.5%
- bound schedulers - 15.8%

This is a very large delta, much bigger than any single throughput improvement we have delivered before on Riak.
",1628897,martinsumner,0,,[],[],1116282482,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
61,21598409042,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-03 16:50:41,1224275889,1826,,"However, is this throughput improvement likely to be true across other hardware configurations?  Is it likely to exist with different test loads?  It would be useful to dig deeper into why throughput improves under these configurations, to understand if this is a general improvement, or one specific to this configuration.",1628897,martinsumner,0,,[],[],1116324784,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
62,21598648875,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-03 17:05:21,1224275889,1826,,"The test is designed to be primarily CPU limited at first, and then when the load switches to be more GET biased it is expected to be CPU heavy but primarily constrained by Disk I/O.

Looking at total CPU used across the nodes (sys, wait and user):

<img width=""1659"" alt=""CPUTime_24hr"" src=""https://user-images.githubusercontent.com/1628897/166502151-3b5fe32a-b857-4af9-b413-0bfd52cca2b7.png"">

So the indication is that with a reduced scheduler count, more work is being done, but with the less or same CPU.  With binding of schedulers, more work is being done, but with more use of CPU (although throughput per CPU cycle is still better than with the default setting towards the backend of the test).

One obvious difference in how the CPU is being used is the number of context switches:

<img width=""1657"" alt=""ContextSwitches_24hr"" src=""https://user-images.githubusercontent.com/1628897/166502639-86eae7bc-9bc1-442c-a4e2-0ff2786e5208.png"">

Either binding schedulers to CPU cores, or reducing the number of schedulers, has a dramatic difference in the number of context switches required.

But what is the cost of a context switch?  This seems a little unclear, as although there is some data available on this context, it is difficult to know if this data is relevant to modern OS/CPU architecture - in particular efficiency savings made related to flushing TLBs.

I suspect there are two basic costs:

- A small, o(1) microsecond cost for each switch in terms of CPU time.
- Some cost in terms of the L1/L2 CPU cache efficiency, particularly where schedulers are bing switched across cores or processor boundaries.  
",1628897,martinsumner,0,,[],[],1116338118,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
63,21598799023,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-03 17:14:51,1224275889,1826,,"As well as increased throughput, one of the biggest headline gains in performance across the different tests, is the improvement in GET latency:

<img width=""1656"" alt=""MeanGETTime_24hr"" src=""https://user-images.githubusercontent.com/1628897/166503613-0a1bec68-7b28-4453-b909-03d34694e4de.png"">

There are multiple different parts of the GET process where speed has improved.

The slot_fetch_time is the time within a SST file in the penciller to read the HEAD of an object out of a block.  The block must be read from disk (which will almost certainly be a read from the VFS page cache), and go through a binary_to_term conversion which will include a zlib decompression, followed by a `lists:nth/2` call on the small (normally 32 item) list that has been converted.  The average time for slot_fetch through the test (here in microseconds), is dramatically different between the configurations:  

<img width=""1659"" alt=""SSTSlotFetchTimeSST12US_24hr"" src=""https://user-images.githubusercontent.com/1628897/166503991-10e23a71-f5db-4b06-88dc-42e9fa8fbe2d.png"">

The slot_fetch is the key part of the HEAD part of the GET process.  The actual GET (which will occur on 1 not 3 vnodes) has 2 parts in the cdb file.  The first part is an index lookup, which requires a calculation of the dj bernstein special has of the key/SQN, and then a file position to read the integers in that position on the index (which will almost certainly be in the page cache).  The average time for the index fetch (here in microseconds), varies significantly between the configurations.
 
<img width=""1656"" alt=""CDBIndexFetchTimeCDB19US_24hr"" src=""https://user-images.githubusercontent.com/1628897/166504458-7bb0b52a-18d4-46f7-a425-903e9dd948fd.png"">

The final part is the actual reading of the object from the disk (which may or may not be in the page cache).  This again varies in line with the headline latency changes (measured here in milliseconds):

<img width=""1658"" alt=""CDBObjFetchTimeCDB19MS_24hr"" src=""https://user-images.githubusercontent.com/1628897/166504691-70183ed3-ecc2-4e6c-b234-7a7048bd1a44.png"">


",1628897,martinsumner,0,,[],[],1116346428,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
64,21598914744,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-03 17:22:06,1224275889,1826,,"All of these processes involve both some CPU-related activity and some interaction with the virtual file system (and in some cases possibly the underlying disk).

If we compare these timings to the timings of larger CPU-only tasks, a difference emerges.

Firstly, looking at the time need to create the slot_hashlist when writing a new SST file.  This is a CPU-only activity, but a background task, i.e. not directly related to any end-user latency.  The timings of these in milliseconds:

<img width=""1659"" alt=""SSTSlotHashListSST13MS_24hr"" src=""https://user-images.githubusercontent.com/1628897/166505129-4fcfacf5-3db9-4fd1-9665-e16b4fdf43da.png"">

Intriguingly, once in the backend of the test there is no improvement in the time for this task between reduced and default scheduler counts.  However, there is a clear improvement when the schedulers are bound to CPUs.

Secondly, looking at the hashtree computation (milliseconds) in the leveled Inker (an infrequent background process) we can see a small gain, but only through scheduler binding: 

<img width=""1656"" alt=""CDBHashtreeComputeCDB07MS_24hr"" src=""https://user-images.githubusercontent.com/1628897/166505638-db99e9de-e717-49e9-8fb9-ea9d8aa74b8c.png"">

",1628897,martinsumner,0,,[],[],1116352828,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
65,21598975803,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-03 17:25:53,1224275889,1826,,"On the write side, when an object is PUT there is a process to update the Bookie's memory, which is a CPU only process.  Timings here in microseconds:

<img width=""1653"" alt=""BookieMemWriteTimeB0015US_24hr"" src=""https://user-images.githubusercontent.com/1628897/166506013-9d9e6fab-2466-47bd-b1a7-31c41f3847a6.png"">

This shows the greatest latency improvement with scheduler binding.

The second, and slower phase is the writing of the object within the inker, which includes some CPU work but also an interaction with the page cache.  Timings here again in microseconds:

<img width=""1657"" alt=""InkerWriteTimeB0015US_24hr"" src=""https://user-images.githubusercontent.com/1628897/166506285-1834f405-966b-48bc-b8fb-48af47f0d0e9.png"">

Now with the I/O interaction the big improvement is related to the reduction in scheduler counts.",1628897,martinsumner,0,,[],[],1116356151,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
66,21599067552,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-03 17:31:27,1224275889,1826,,Overall there seems to be a pattern.  Pure CPU work is generally made faster by binding CPUs to schedulers.  Work that interacts with the VFS is made faster/more-efficient by reducing the count of schedulers. ,1628897,martinsumner,0,,[],[],1116362579,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
67,21599187936,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-03 17:39:06,1224275889,1826,,"But why would reducing the scheduler count have this impact?

If we look at the actual underlying volume of data being written and read from the disk, it would be reasonable to expect that this will vary between the test runs in-line with the throughput.  Looking at write KB per second (from disk):

<img width=""1657"" alt=""WritesKBpersec_24hr"" src=""https://user-images.githubusercontent.com/1628897/166507555-4439f55d-a314-4eea-b2df-b4bebcea8c29.png"">

The alignment between write volume and throughput appears to be roughly present as expected.

However, looking at read KBs per second (from disk):

<img width=""1657"" alt=""ReadsKBpersec_24hr"" src=""https://user-images.githubusercontent.com/1628897/166507826-c1d80ba6-6d81-4977-abcb-319d601df9e0.png"">

Now the alignment doesn't appear to exist, especially with reduced scheduler counts.  With reduced scheduler counts, there must be more reading from the VFS (as more throughput), but this is achieved with less reading form actual disk.  This is also true (but to a lesser extent) with scheduler binding.

This would imply that the VFS page cache is being used much more in these cases.  But how are scheduler changes making the VFS page cache more efficient.

Looking at the reported memory deployed by the VFS page cache - there is not an obvious difference here:

<img width=""1655"" alt=""CacheMemory_24hr"" src=""https://user-images.githubusercontent.com/1628897/166508335-552e6600-4cb2-4bf9-8dd0-408939eadf8b.png"">

So the apparent improved VFS page cache efficiency with reduced schedulers does not have an obvious or easy explanation.

",1628897,martinsumner,0,,[],[],1116369758,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
68,21599282726,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-03 17:45:04,1224275889,1826,,"In summary, there is a performance test where we improve throughput by 22.5% by reducing the scheduler count, and get an improvement of 15.8% by binding schedulers to CPUs.

The reason why seems relatively obvious and expected for scheduler binding.  Reduced context switching improve efficiency of CPU centric tasks.

The reason why seems strange and mysterious for the reduced scheduler count.  Yes, there are some CPU efficiency improvements potentially related to reduced context switching, but the biggest improvement appears to be in the efficiency of the VFS page cache on both reads and writes.",1628897,martinsumner,0,,[],[],1116375205,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
69,21599575790,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-03 18:03:17,1224275889,1826,,"Scheduler binding is a known way to improve performance in Erlang systems., and is advertised as such within the erlang documentation.

In [RabbitMQ](https://www.rabbitmq.com/runtime.html#scheduler-bind-type), it is now the default to use scheduler binding.

However, the side effects associated with other activity on the same node can be severe.  In RabbitMQ this is mitigated that nodes used for RabbitMQ are dedicated to that purpose.

The same mitigation could be stipulated for Riak.  However, even if no application workloads co-exist on the same node, all Riak nodes will have operational software (e.g. monitoring, security, backups etc).  That operational software may have a limited throughput when working correctly - but may also have error conditions where they work in unexpected ways.

Overall, the potential side effects of scheduler binding seem out-of-step with the primary aim of Riak to be reliable in exceptional circumstances (as a priority over performant).",1628897,martinsumner,0,,[],[],1116391390,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
70,21599613668,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-03 18:05:37,1224275889,1826,,"Reducing scheduler counts appears to be a safer way to improve performance, but without a full understanding of why it is improving performance, it doesn't seem to correct to make this a default setting.  This is especially true, as what might be an improvement with a full beam-based backend like leveled, may not be true with NIF-based backends like eleveldb/bitcask (where dirty scheduler improvements have not been made). ",1628897,martinsumner,0,,[],[],1116393464,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
71,21600228409,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-03 18:42:47,1224275889,1826,,"One interesting aside is - if the throughput improvements for scheduler binding and scheduler reductions are not tightly correlated, what would happen if the two changes were combined?  More throughput improvements?

See below for the log-scale throughput chart, this time with a green line showing the combination of the two changes:

<img width=""1578"" alt=""ThroughputComparison_24hr_LogScale_withCombo"" src=""https://user-images.githubusercontent.com/1628897/166520385-46f363ea-8b53-4cff-aa50-b275a3588330.png"">

In the initial part of the test - there is an improvement over and above what can be achieved through either setting.  However, towards the end of the test, the combined setting has lower throughput than either individual setting.  At the 250M update mark, the throughput improvement is just 4.7% more than the default.

This is hard to explain, but it is noticeable that the I/O & page cache related improvements appear to reverse when reduced schedulers is combined with bound schedulers.

The throughput towards the end of the test is much more important than at the beginning - most Riak cluster spend most of their time processing requests with a large set of data already present.  So combining the two changes, does not seem to be as good as making either one of the changes.",1628897,martinsumner,0,,[],[],1116439732,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
72,21600609924,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-03 19:07:05,1224275889,1826,,"One thing of potential interest to emerge in [this thread](https://twitter.com/MackeyTech/status/1521236821933395968) is the `sbwt` setting.

The reason described by @seancribbs for changing this are related specifically to containerised environments, but perhaps it is worth adding this to the `riak.conf` tuneable parameters, and experiment further to see if there is any influence on context switching through reduction of buy waiting. 

It was noticeable during testing of the combined binding/reduced setup that the ""Other"" count was dominant form of busyness on the standard schedulers:

```
       Thread      aux check_io emulator       gc    other     port    sleep

Stats per thread:
     async( 0)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
     async( 1)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
     async( 2)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
     async( 3)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
       aux( 1)    1.43%    0.50%    0.00%    0.00%    0.17%    0.00%   97.90%
dirty_cpu_( 1)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_( 2)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_( 3)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_( 4)    0.00%    0.00%    0.00%    1.74%    0.09%    0.00%   98.17%
dirty_cpu_( 5)    0.00%    0.00%    0.02%    6.09%    0.45%    0.00%   93.44%
dirty_cpu_( 6)    0.00%    0.00%    0.01%    2.70%    0.20%    0.00%   97.10%
dirty_cpu_( 7)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_( 8)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_( 9)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_(10)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_(11)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_(12)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_io_s( 1)    0.00%    0.00%   75.61%    0.00%    5.27%    0.00%   19.12%
dirty_io_s( 2)    0.00%    0.00%   74.64%    0.00%    5.65%    0.00%   19.71%
dirty_io_s( 3)    0.00%    0.00%   74.86%    0.00%    4.95%    0.00%   20.19%
dirty_io_s( 4)    0.00%    0.00%   76.37%    0.00%    5.07%    0.00%   18.56%
dirty_io_s( 5)    0.00%    0.00%   71.89%    0.00%    5.48%    0.00%   22.63%
dirty_io_s( 6)    0.00%    0.00%   73.23%    0.00%    5.41%    0.00%   21.36%
dirty_io_s( 7)    0.00%    0.00%   72.72%    0.00%    5.23%    0.00%   22.05%
dirty_io_s( 8)    0.00%    0.00%   76.20%    0.00%    5.07%    0.00%   18.73%
dirty_io_s( 9)    0.00%    0.00%   72.99%    0.00%    5.40%    0.00%   21.60%
dirty_io_s(10)    0.00%    0.00%   75.00%    0.00%    5.50%    0.00%   19.50%
      poll( 0)    0.00%    0.61%    0.00%    0.00%    0.00%    0.00%   99.39%
 scheduler( 1)    1.78%    0.31%   24.80%    2.31%   29.83%    1.39%   39.58%
 scheduler( 2)    1.75%    0.32%   23.67%    2.45%   30.19%    1.43%   40.19%
 scheduler( 3)    1.79%    0.34%   25.18%    2.34%   30.60%    1.48%   38.27%
 scheduler( 4)    1.72%    0.33%   23.94%    2.30%   29.71%    1.49%   40.51%
 scheduler( 5)    1.74%    0.31%   24.15%    2.37%   29.96%    1.44%   40.03%
 scheduler( 6)    1.78%    0.30%   23.26%    2.19%   29.81%    1.38%   41.28%
 scheduler( 7)    1.91%    0.35%   26.51%    2.58%   29.54%    1.71%   37.40%
 scheduler( 8)    1.95%    0.34%   27.67%    2.75%   29.31%    1.69%   36.29%
 scheduler( 9)    1.86%    0.35%   26.29%    2.55%   29.39%    1.71%   37.85%
 scheduler(10)    1.94%    0.36%   27.61%    2.76%   29.44%    1.76%   36.12%
 scheduler(11)    1.92%    0.35%   27.22%    2.71%   29.06%    1.74%   37.00%
 scheduler(12)    1.90%    0.37%   27.15%    2.65%   29.53%    1.84%   36.56%
 scheduler(13)    1.74%    0.32%   23.98%    2.30%   29.97%    1.45%   40.25%
 scheduler(14)    1.76%    0.32%   23.72%    2.31%   29.97%    1.46%   40.46%
 scheduler(15)    1.77%    0.34%   24.33%    2.30%   30.32%    1.50%   39.46%
 scheduler(16)    1.79%    0.33%   25.17%    2.14%   29.90%    1.49%   39.19%
 scheduler(17)    1.81%    0.32%   25.50%    2.27%   30.29%    1.54%   38.27%
 scheduler(18)    1.59%    0.29%   21.21%    1.92%   28.39%    1.26%   45.34%
 scheduler(19)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
 scheduler(20)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
 scheduler(21)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
 scheduler(22)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
 scheduler(23)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
 scheduler(24)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%

Stats per type:
         async    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
           aux    1.43%    0.50%    0.00%    0.00%    0.17%    0.00%   97.90%
dirty_cpu_sche    0.00%    0.00%    0.00%    0.88%    0.06%    0.00%   99.06%
dirty_io_sched    0.00%    0.00%   74.35%    0.00%    5.30%    0.00%   20.35%
          poll    0.00%    0.61%    0.00%    0.00%    0.00%    0.00%   99.39%
     scheduler    1.35%    0.25%   18.81%    1.80%   22.30%    1.16%   54.33%
```

[More information on this potential change](https://stressgrid.com/blog/beam_cpu_usage/).   ",1628897,martinsumner,0,,[],[],1116461580,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
73,21601272901,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-03 19:50:41,1193014932,1820,,"A summary of the current understanding of memory issues.

- Observations had been made of Riak clusters rapidly escalating the amount of memory being taken by the beam process, and ending in a state with a high proportion of memory fragmentation.

- There appear to be two relevant factors.  This appears to be more likely to occur on clusters with large (in terms of key size) leveled-backed vnodes.  This appears to occur more frequently with pure-PUT loads, for example in transfers or in cluster full-syncs to fresh clusters (with leveled-backed vnodes).

- There is an issue with caches in leveled_sst files.  Those caches are formed on activity, and never dropped.  The larger the count of files, the more is needed by cache.  If the file is largely unused (i.e. it contains stale data) this cache is a waste of resource, but it will still be formed due to SQN checks in the Journal compaction process.

- There is an issue with back-pressure on PUT loads.  When leveled has a backlog of ledger compaction work (i.e. the LSM tree), rather than stall, it will temporarily hold more data in memory to continue, while infrequently adding small latency to PUT responses.  Leveled is bending rather than breaking, assuming pressure will eventually lift .... but it may not eventually lift and more and more memory may be required, particularly where there is not GET traffic (which naturally directs requests away from busy vnodes).

- When pressure is relieved altogether, leveled without a trigger of new PUTs will continue to hold data in memory.  Also there is no de-fragmentation process in the VM, so without any turnover of memory the VM's memory allocation will remain fragmented.

The following improvements are therefore to be added to 3.0.10:

1. When a SQN check is processed by a leveled_sst file, it will set a timeout, and if there are no further requests at the timeout the caches will be flushed, and the process will hibernate.  This means that the background journal compaction process will flush as well as build caches on infrequently accessed files, and prompt churn of memory

2. The frequency of ""slow-offers"", artificial delays on PUTs will be increased during work backlogs.  This frequency will be tuneable, as will the pause length (which is already tuneable).

3. There will now be prompted garbage collection on both the penciller and penciller's clerk processes immediately after infrequent events which touch a large number of binaries on the shared heap.

4. The VM memory settings will be exposed via `riak.conf`, but no defaults will be changed.  Currently, it is considered preferable to resolve these issues by changing the code rather than trying to tune the VM.  This is not really an issue with the VM and fragmentation, the fragmentation is symptom of poor practice within the leveled code. 

5. There MAY be a change to prompt regularly to flush memory to disk when a backlog occurs, even if no new PUTs arrive.
",1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],1116501790,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
74,21602608357,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-03 21:11:06,1224275889,1826,,"A further note on the combination of reduced scheduler counts and binding.  When this combined setting is in place, the CPU bound activity gets faster than just having the normal number of CPUs bound.  However, any impact related improved interaction with the page cache is entirely negated.

Hence we see in the curve, throughput improved while the cluster is primarily CPU bound, but throughput worse when the cluster is more disk bound. ",1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],1116661651,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
75,21758707339,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 16:24:41,1234204942,1829,,https://github.com/basho/riak_kv/issues/1817,1628897,martinsumner,0,,[],[],1125192187,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
76,21762724425,IssueCommentEvent,created,1933698,s2hc-johan,791522,basho/riak_kv,176293,basho,2022-05-12 20:36:39,1234344008,1830,,"Looks good, I think",1628897,martinsumner,0,,[],[],1125399837,1933698,s2hc-johan,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
77,21841156842,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-17 21:03:27,1224275889,1826,,"The tests have been re-run with some subtle changes to the load, and over a longer test (48 hours).  Towards the end of this test, disk busyness is almost totally dominant in determining throughput.

There are three variants from defaults tested:

- Reduced scheduler counts
- Bound schedulers
- Disabled Busy wait threshold

Looking at the results with different VM/scheduler settings - improvement sin throughput in the mid-part of the test are seen with all variants:

<img width=""1656"" alt=""ThroughputComparison_48h"" src=""https://user-images.githubusercontent.com/1628897/168908902-3c47f8aa-030d-4a52-8795-95d900c9f7f5.png"">

What we can see in all variants improvements in response times, especially GET:

<img width=""1656"" alt=""GetTime_48h"" src=""https://user-images.githubusercontent.com/1628897/168909234-477553fe-5a96-41f8-a9a6-46d64ae9e83e.png"">

What is  noticeable, is the difference in CPU utilisation between the settings.  The test is run on a cluster of 12-core (24 vCPU) servers.  Disabling the busy wait gives throughput improvements whilst running 3.4 vCPU less on average  compared to VM defaults.  This is a significant improvement in efficiency:

<img width=""1653"" alt=""CPUutilisation_48h"" src=""https://user-images.githubusercontent.com/1628897/168909797-cd198043-30da-4a89-90d4-66f925771f90.png"">

",1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],1129315233,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
78,21959486420,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-24 14:42:51,1211221948,1825,,"The multiplier has been changed.

In testing this change, the underlying problem was revealed - https://github.com/martinsumner/leveled/pull/377

PR to be included in 3.0.10",1628897,martinsumner,0,,[],[],1136021464,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
79,21978558446,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-25 11:13:56,1224275889,1826,,"Note RabbitMQ has gained improvement through scheduler binding - https://github.com/rabbitmq/rabbitmq-server/issues/612.

However, what is the impact of this change if there are other processes running on the same machine (at the NHS there was an incident related to scheduler binding on RabbitMQ - although in this case RabbitMQ was not being run  on a dedicated machine in line with Rabbit guidance).  Riak guidance is also to run Riak on dedicated machines, but even if the machine is ""dedicated"" there will still be alternative processes for operational reasons (e.g. monitoring), and it is not possible that such processes will always behave as expected.  So I'm wary of scheduler binding as a default approach.

My preference would be disable busy-wait as the recommended change, primarily because of the CPU efficiency improvements we see.  However there exists a specific warning in the docs that this flag may not be available in future releases. ",1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],1137112316,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
80,21999859309,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-26 10:28:25,1234344008,1830,,Related to https://github.com/basho/riak_kv/pull/1814,1628897,martinsumner,0,,[],[],1138388520,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
81,22061715406,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-30 18:53:59,1193014932,1820,,"1 - 4 are implemented in riak 3.0.10.

Also in 3.0.10, another significant cause of memory issues in leveled was detected - https://github.com/martinsumner/leveled/pull/377.

With these fixes, fix (5) has been deferred - https://github.com/martinsumner/leveled/pull/376.

It does look very likely that memory issues are not VM-related, they are all in effect leveled bugs.

Also in 3.0.10 are a series of fixes to use disk-backed queues rather than pure memory queues to avoid another area where memory can run out of control.",1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],1141414135,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
82,25694871411,IssueCommentEvent,created,1107079,hmmr,791522,basho/riak_kv,176293,basho,2022-12-06 16:00:47,1462321713,1838,,+1,1628897,martinsumner,0,,[],[],1339597782,1107079,hmmr,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
83,25915718662,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-15 17:11:12,1208273405,1824,,"If a rebuild fold is interrupted by a `riak stop` ... then as the node shuts down a number of key reads will be falsely report corruption, as the FoldObjectsFun will throw {error, badarg} when trying to get_index_n.

We may get an {error, badarg} due to corruption ... so shouldn't just not log.  Either need to detect that riak is stopping, or add a disclaimer to ignore if shutting down.  The log may otherwise cause unnecessary concern for operators.
",1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],1353419522,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
84,25916576352,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-15 17:52:24,1208273405,1824,,"https://github.com/basho/riak_kv/pull/1840

On reflection, simply making the log clearer about context should be sufficient",1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],1353486846,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
85,25920950302,IssueCommentEvent,created,26117856,azharnisar,791522,basho/riak_kv,176293,basho,2022-12-15 21:53:33,1498852947,1840,,+1,1628897,martinsumner,0,,[],[],1353756353,26117856,azharnisar,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
86,25993405322,IssueCommentEvent,created,1583029,systream,791522,basho/riak_kv,176293,basho,2022-12-20 09:35:45,1443506535,1836,,Thanks. ,1583029,systream,0,,[],[],1359076621,1583029,systream,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
87,25999040432,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-20 13:58:13,1442034005,1835,,Included in Riak 3.0.12,1583029,systream,0,,[],[],1359399880,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
88,22081659216,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-31 18:05:36,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,10032637674,e21ab69e9270386205e1c5a87a37a953a1f68f35,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Update rebar.config'],0,0,,,,0,,,0,,,[]
89,22083216024,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-31 19:32:41,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,10033336620,a632efeb773282b2c203e587dff2956eb6f34457,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Mas 310 merge (#1832)\n\nMerge in changes from Riak 3.0.10.\r\n\r\nIncludes PRs:\r\n\r\n- https://github.com/basho/riak_kv/pull/1809\r\n- https://github.com/basho/riak_kv/pull/1812\r\n- https://github.com/basho/riak_kv/pull/1814\r\n- https://github.com/basho/riak_kv/pull/1816\r\n- https://github.com/basho/riak_kv/pull/1829\r\n- https://github.com/basho/riak_kv/pull/1830'],0,0,,,,0,,,0,,,[]
90,19604223129,WatchEvent,started,7991350,kangkot,791522,basho/riak_kv,176293,basho,2022-01-06 07:08:34,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
91,19628248406,WatchEvent,started,16581931,v1d3rm3,791522,basho/riak_kv,176293,basho,2022-01-07 16:08:16,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
92,22991066778,WatchEvent,started,54551308,lazypwny751,791522,basho/riak_kv,176293,basho,2022-07-20 20:42:54,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
93,23173707858,WatchEvent,started,5423199,laplaceliu,791522,basho/riak_kv,176293,basho,2022-07-31 04:56:54,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
94,20622185212,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-07 18:31:23,1161720703,1813,,"Initial thoughts are, that the best solution is to change the riak_kv_get_fsm so as not to trigger delete on fetch (in the riak_kv_get_fsm).  This represents a change from `riak_repl` (as `riak_repl` was push rather than pull-based, and so did not use fetch), and hence why we have not seen this before.

This should prevent the cycle.

There is perhaps also a question about whether it is right to consider new_actor_epoch on tombstones when delete_mode is not keep.  Should key amnesia be the expectation here with replicated tombstones due to automatic reaping? 

I think the safest thing is to leave key amnesia as it was, as it appears to be only this replication scenario that can trigger this cycle.",1628897,martinsumner,0,,[],[],1060999671,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
95,20627266843,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-08 00:29:41,1161720703,1813,,"changing the fetch behaviour may cause other problems.  In particular, if we have the same situation - but this time there have bene no previous coordinate PuTs on either cluster (it originates from ClusterC).

There could now be a rotation, whereby the tombstone is on A.  Full-sync prompts A -> B.  There is no amnesia on  B, and so the GET post PUSH on B will now prompt the reap.  However, the Fetch from A hasn't prompted a reap now ... so we are just rotating tombstones again.",1628897,martinsumner,0,,[],[],1061281085,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
96,20627309239,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-08 00:33:41,1161720703,1813,,There is a demonstration of the original problem in [this test](https://github.com/basho/riak_test/commit/1e681da5a81c66cf18f3ce42fd0fe72c9dbdb9a1),1628897,martinsumner,0,,[],[],1061283347,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
97,20634161799,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-08 10:06:08,1161720703,1813,,"The kv679 behaviour, or handling key amnesia in all delete_modes, is implicitly tested (in that the default non-keep mode is used in all the tests).  So I don't think it is potentially unsafe to change this behaviour.

The alternative is to end the rule of a single final action only in riak_kv_get_fsm.  If a repair is prompted, and the repaired object is a tombstone, it should also prompt a delete (which is really a reap request).",1628897,martinsumner,0,,[],[],1061607670,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
98,20644967016,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-08 20:07:52,1161720703,1813,,https://github.com/basho/riak_kv/pull/1814,1628897,martinsumner,0,,[],[],1062163157,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
99,20661455906,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-09 15:21:05,1163069162,1814,,"@martincox - you may want to check this.  It is to fix a `nextgenrepl` issue, but will change functionality in other systems.  It is a very subtle change in the GET_FSM behaviour following GET.

Previously after completing a GET, only one action (other than `nop`) could be performed: `read_repair` or `delete` (i.e. trigger tombstone reaping).  If the GET revealed that both need to occur (i.e. there needs to be a repair, and also the winning object is a tombstone so tombstone trigger also required), only the repair would be done.

This indirectly led to a weird [issue](https://github.com/basho/riak_kv/issues/1813), which could cause some serious problems.

Now, as well as `read_repair` and `delete`, an additional `delete_repair` option can occur in this circumstance.  If `delete_repair` is triggered, the FSM will first repair then delete.  So a fetch will always trigger delete when required.
",1628897,martinsumner,0,,[],[],1063033264,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
100,20662764450,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-09 16:23:13,1164153317,1815,,From a configuration perspective the window should follow the model of the bitcask merge window.,1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],1063106903,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
101,20668679262,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-09 22:25:21,1164504475,1816,,https://github.com/basho/riak_test/pull/1364,1628897,martinsumner,0,,[],[],1063433705,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
102,20668686901,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-09 22:25:57,1164153317,1815,,https://github.com/basho/riak_kv/pull/1816,1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],1063434110,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
103,20668694456,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-09 22:26:33,1164504475,1816,,Awaiting documentation and schema changes,1628897,martinsumner,0,,[],[],1063434496,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
104,20739357508,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-14 16:17:03,1168609024,1818,,https://github.com/basho/riak_kv/issues/1817,1628897,martinsumner,0,,[],[],1067018088,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
105,20871265670,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-22 12:11:56,1176654692,1819,,https://github.com/basho/riak_kv/pull/1814,1628897,martinsumner,0,,[],[],1075098874,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
106,25915718836,IssuesEvent,reopened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-15 17:11:12,1208273405,1824,,,1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
107,25999020907,IssuesEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-20 13:57:24,1224275889,1826,,,1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
108,25999027623,IssuesEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-20 13:57:41,1208273405,1824,,,1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
109,25999040530,IssuesEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-20 13:58:13,1442034005,1835,,,1583029,systream,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
110,21120429790,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-05 12:42:19,1193014932,1820,,"Changing the single block carrier threshold to a lower value on the eheap_alloc has a very negative impact.  This change does not help

erlang.eheap_memory.sbct = 128

```
recon_alloc:memory(usage).
0.34210991984944206
(dev11@192.168.6.11)5> erlang:memory().          
[{total,7731430560},
 {processes,6054481520},
 {processes_used,6054116632},
 {system,1676949040},
 {atom,1017017},
 {atom_used,1009277},
 {binary,1307030592},
 {code,15398935},
 {ets,300506192}]
(dev11@192.168.6.11)6> recon_alloc:memory(allocated).
22463975424
(dev11@192.168.6.11)7>  rp(recon_alloc:fragmentation(current)).
[{{eheap_alloc,7},
  [{sbcs_usage,0.21770690720393554},
   {mbcs_usage,0.6661469330658784},
   {sbcs_block_size,384285504},
   {sbcs_carriers_size,1765150720},
   {mbcs_block_size,16152944},
   {mbcs_carriers_size,24248320}]},
 {{eheap_alloc,23},
  [{sbcs_usage,0.23321187683249425},
   {mbcs_usage,0.5346889303188131},
   {sbcs_block_size,249886832},
   {sbcs_carriers_size,1071501312},
   {mbcs_block_size,6938192},
   {mbcs_carriers_size,12976128}]},
 {{eheap_alloc,1},
...
```",1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],1088656816,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
111,21140571087,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-06 10:54:23,1193014932,1820,,"Testing shows that in some cases, either reducing the lmbcs (in this case to 1280KB), or setting ramv to true for both binary and eheap allocators can reduce the memory footprint of Riak (by better controlling fragmentation):

<img width=""1655"" alt=""MemoryFragment_Memory"" src=""https://user-images.githubusercontent.com/1628897/161959281-e9783411-bc57-4fe2-a01c-fc85630aea46.png"">

However, the change to enable ramv comes at a CPU cost (as expected):

<img width=""1654"" alt=""MemoryFragment_CPU"" src=""https://user-images.githubusercontent.com/1628897/161959415-f80ae986-435a-4e0c-9b7c-17dc2cb4a7c3.png"">

",1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],1090132249,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
112,21263522003,IssueCommentEvent,created,5654175,maciejbiesek,791522,basho/riak_kv,176293,basho,2022-04-13 07:42:48,1200477764,1821,,Being more precise: links to download Ubuntu version have exprired,5654175,maciejbiesek,0,,[],[],1097664625,5654175,maciejbiesek,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
113,21291555756,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-14 14:25:53,1193014932,1820,,"The memory manipulation had an impact, but a relatively minor impact.  Most of the memory being used by the BEAM in this test is as a result of leveled consuming memory for caches.  Testing with a change to the leveled_sst cache to periodically release the blockindex_cache when not busy (and also have more GC triggers within the code), changing SST caching has a bigger difference:

<img width=""1658"" alt=""MemoryCompare_HostMemFootprint"" src=""https://user-images.githubusercontent.com/1628897/163409769-3f78ee2b-fcda-4c08-b15e-a6a76f49278c.png"">

Get requests are unevenly spread around the cluster with leveled, the GET request will be directed to the node which responded fastest to the HEAD request.  So by examining how the GET requests were distributed, we can see which nodes were performing better during the test:

<img width=""1656"" alt=""MemoryCompare_GetShare"" src=""https://user-images.githubusercontent.com/1628897/163410067-f9afedfa-bbe8-4c3f-8794-8a8fccde5733.png"">

Now looking at CPU by node, there is very little difference, so the ""sst+ramv"" node was doing more work and doing that work with relative efficiency:

<img width=""1656"" alt=""MemoryCompare_CPULoad"" src=""https://user-images.githubusercontent.com/1628897/163410782-52ec4c9b-5337-4bc2-9e33-38b0f040e903.png"">

Following this point in the test, one node had issues, and scanning of the full store for either intra or inter cluster AAE increased.  Those nodes with the sst change then showed higher disk utilisation (as block index caches were fetched from disk), but maintained their memory advantage.  Hypothesis is that releasing and regaining the cache has disk costs, but reduces fragmentation.

",1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],1099242549,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
114,21350334073,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-19 09:26:56,1168609024,1818,,Overflow Queues extended to include a 'reader' queue which may be sued for read_repairs.  Currently this queue is only used for the repair_keys_range query and the read-repair triggered by key amnesia.,1628897,martinsumner,0,,[],[],1102379271,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
115,21358770613,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-19 16:35:09,1208273405,1824,,"@s2hc-johan can you have a look at this please?

https://github.com/basho/riak_kv/pull/1823/commits/f54ffa3c657087613b714b1d67e60d8df9c0a860

Does this look like it should solve the problem?",1628897,martinsumner,0,,[],[],1102858634,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
116,21361138124,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-19 18:58:21,1193014932,1820,,"As well as experimenting with memory settings, I've also done some experimentation with scheduler settings.  It was noticeable with changes in scheduler settings I was getting variances in memory usage ... and sometimes improvements when compared to different memory settings.

The scheduler changes have shown that by default too many schedulers are starting, and this is causing a huge number of context switches, and consequently higher CPU (and presumably inefficient usage of memory caches).

What is interesting here is the non-default settings that basho introduced to counter problems with scheduler collapse in R16 seem to be causing issues, and that we get better behaviour with current stock erlang settings.

I have some more tests to run, before I have clear answers, but currently I'm considering if supporting changes here may cause operational overheads ... and overall we're better sticking to well tested beam defaults for whatever version we're running.  When I start looking at all factors combined (memory, CPU use, latency, CPU efficiency), the default settings start to look more desirable. 

Will post-up results as I get them.

@hmmr - so with regards to exposing disabling of allocators, I'm not sure.  I might not expose/test that at this stage .. and in general if I think it looks like moving towards rather than away from beam defaults is the right way to go, I would be cautious about doing it at all.",1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],1102983021,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
117,21361335486,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-19 19:10:36,1193014932,1820,,"One interesting thing that has been seen, is in the test 1 of the 4 nodes is put under greater pressure than the other nodes.  In this case we get this strange situation where msacc shows all schedulers very busy, but the OS does not show the equivalent CPU business.

So on the node top reports the equivalent of about 14 busy CPUs:

```
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                         
 8589 root      20   0 24.686g 0.015t   5476 S  1374 16.5  12725:00 beam.smp   
```

But looking at microstate accounting at this time:

```
(dev11@192.168.6.11)1> msacc:start(10000).
true
(dev11@192.168.6.11)2> msacc:print().
Average thread real-time    :  10005371 us
Accumulated system run-time : 282190754 us
Average scheduler run-time  :   6997919 us

        Thread      aux check_io emulator       gc    other     port    sleep

Stats per thread:
     async( 0)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
     async( 1)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
     async( 2)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
     async( 3)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
       aux( 1)   20.89%    0.51%    0.00%    0.00%    0.16%    0.00%   78.45%
dirty_cpu_( 1)    0.00%    0.00%    0.29%   26.42%    0.87%    0.00%   72.42%
dirty_cpu_( 2)    0.00%    0.00%    0.29%   34.74%    1.16%    0.00%   63.80%
dirty_cpu_( 3)    0.00%    0.00%    0.14%   22.66%    0.69%    0.00%   76.52%
dirty_cpu_( 4)    0.00%    0.00%    0.18%   27.57%    0.87%    0.00%   71.38%
dirty_cpu_( 5)    0.00%    0.00%    0.31%   29.68%    0.91%    0.00%   69.10%
dirty_cpu_( 6)    0.00%    0.00%    0.45%   27.61%    0.95%    0.00%   70.99%
dirty_cpu_( 7)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_( 8)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_( 9)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_(10)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_(11)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_(12)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_io_s( 1)    0.00%    0.00%   69.90%    0.00%    9.17%    0.00%   20.93%
dirty_io_s( 2)    0.00%    0.00%   69.71%    0.00%    8.58%    0.00%   21.72%
dirty_io_s( 3)    0.00%    0.00%   69.07%    0.00%    9.41%    0.00%   21.52%
dirty_io_s( 4)    0.00%    0.00%   69.03%    0.00%    9.05%    0.00%   21.92%
dirty_io_s( 5)    0.00%    0.00%   70.62%    0.00%    8.19%    0.00%   21.19%
dirty_io_s( 6)    0.00%    0.00%   70.78%    0.00%    8.91%    0.00%   20.31%
dirty_io_s( 7)    0.00%    0.00%   70.06%    0.00%    8.72%    0.00%   21.22%
dirty_io_s( 8)    0.00%    0.00%   69.54%    0.00%    8.63%    0.00%   21.84%
dirty_io_s( 9)    0.00%    0.00%   69.41%    0.00%    8.77%    0.00%   21.83%
dirty_io_s(10)    0.00%    0.00%   69.69%    0.00%    8.96%    0.00%   21.35%
dirty_io_s(11)    0.00%    0.00%   69.89%    0.00%    8.32%    0.00%   21.79%
dirty_io_s(12)    0.00%    0.00%   70.52%    0.00%    8.87%    0.00%   20.62%
      poll( 0)    0.00%    0.66%    0.00%    0.00%    0.00%    0.00%   99.34%
 scheduler( 1)    4.48%    0.19%   62.99%   21.05%    3.40%    0.80%    7.08%
 scheduler( 2)    4.86%    0.19%   62.64%   21.34%    3.53%    0.81%    6.62%
 scheduler( 3)    4.38%    0.20%   63.29%   21.45%    3.36%    0.81%    6.52%
 scheduler( 4)    4.23%    0.19%   62.81%   21.40%    3.55%    0.86%    6.96%
 scheduler( 5)    4.30%    0.21%   63.01%   21.36%    3.53%    0.76%    6.83%
 scheduler( 6)    4.32%    0.22%   64.29%   20.29%    3.36%    0.66%    6.86%
 scheduler( 7)    4.48%    0.23%   63.57%   20.68%    3.45%    0.89%    6.70%
 scheduler( 8)    4.31%    0.18%   63.17%   21.52%    3.36%    0.79%    6.68%
 scheduler( 9)    4.14%    0.22%   63.84%   20.83%    3.42%    0.88%    6.67%
 scheduler(10)    4.14%    0.19%   63.47%   21.39%    3.48%    0.78%    6.54%
 scheduler(11)    4.30%    0.19%   63.75%   20.51%    3.43%    0.80%    7.03%
 scheduler(12)    4.35%    0.22%   62.91%   21.62%    3.52%    0.77%    6.61%
 scheduler(13)    4.19%    0.24%   61.97%   21.96%    3.63%    0.97%    7.03%
 scheduler(14)    4.74%    0.23%   61.25%   22.48%    3.62%    0.82%    6.87%
 scheduler(15)    4.27%    0.18%   63.32%   21.58%    3.32%    0.71%    6.63%
 scheduler(16)    4.24%    0.19%   62.98%   21.84%    3.33%    0.82%    6.59%
 scheduler(17)    4.39%    0.20%   63.12%   21.42%    3.60%    0.77%    6.50%
 scheduler(18)    4.49%    0.20%   63.63%   20.76%    3.54%    0.75%    6.62%
 scheduler(19)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
 scheduler(20)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
 scheduler(21)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%   99.99%
 scheduler(22)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
 scheduler(23)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
 scheduler(24)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%

Stats per type:
         async    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
           aux   20.89%    0.51%    0.00%    0.00%    0.16%    0.00%   78.45%
dirty_cpu_sche    0.00%    0.00%    0.14%   14.06%    0.45%    0.00%   85.35%
dirty_io_sched    0.00%    0.00%   69.85%    0.00%    8.80%    0.00%   21.35%
          poll    0.00%    0.66%    0.00%    0.00%    0.00%    0.00%   99.34%
     scheduler    3.28%    0.15%   47.33%   15.98%    2.60%    0.60%   30.06%
```

There are 18 schedulers that are only sleeping around 6% of the time, and 12 dirty_io schedulers that are only sleeping 20% of the time ... which amounts to more than 2600% CPU!!!

When the node is in this state, although it is reporting a lot of CPU time spent on GC, it isn't clear GC is occurring as the largest processes my memory usage have large amounts of collectable garbage.  It is in this state that we can see the memory usage by the beam on the node increasing.",1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],1102993908,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
118,21361367740,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-19 19:12:39,1193014932,1820,,"during the same test, at the same time, on a ""healthy"" node in the cluster:

```
PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                         
36026 root      20   0 26.183g 0.017t   5480 S  1145 18.1  11528:10 beam.smp 
```

```
(dev12@192.168.6.12)1> msacc:start(10000).
true
(dev12@192.168.6.12)2> msacc:print().
Average thread real-time    :  10002026 us
Accumulated system run-time : 178505482 us
Average scheduler run-time  :   4410064 us

        Thread      aux check_io emulator       gc    other     port    sleep

Stats per thread:
     async( 0)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
     async( 1)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
     async( 2)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
     async( 3)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
       aux( 1)    4.08%    0.34%    0.00%    0.00%    0.11%    0.00%   95.47%
dirty_cpu_( 1)    0.00%    0.00%    0.07%    4.70%    0.20%    0.00%   95.02%
dirty_cpu_( 2)    0.00%    0.00%    0.05%    2.38%    0.13%    0.00%   97.44%
dirty_cpu_( 3)    0.00%    0.00%    0.02%    3.63%    0.18%    0.00%   96.17%
dirty_cpu_( 4)    0.00%    0.00%    0.07%    2.70%    0.16%    0.00%   97.07%
dirty_cpu_( 5)    0.00%    0.00%    0.00%    0.29%    0.02%    0.00%   99.69%
dirty_cpu_( 6)    0.00%    0.00%    0.05%    3.09%    0.14%    0.00%   96.72%
dirty_cpu_( 7)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_( 8)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_( 9)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_(10)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_(11)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_cpu_(12)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
dirty_io_s( 1)    0.00%    0.00%   52.39%    0.00%    4.63%    0.00%   42.98%
dirty_io_s( 2)    0.00%    0.00%   55.88%    0.00%    4.71%    0.00%   39.41%
dirty_io_s( 3)    0.00%    0.00%   53.49%    0.00%    4.67%    0.00%   41.84%
dirty_io_s( 4)    0.00%    0.00%   59.09%    0.00%    5.33%    0.00%   35.59%
dirty_io_s( 5)    0.00%    0.00%   57.04%    0.00%    4.62%    0.00%   38.34%
dirty_io_s( 6)    0.00%    0.00%   51.64%    0.00%    4.71%    0.00%   43.65%
dirty_io_s( 7)    0.00%    0.00%   48.15%    0.00%    4.25%    0.00%   47.61%
dirty_io_s( 8)    0.00%    0.00%   47.58%    0.00%    4.91%    0.00%   47.51%
dirty_io_s( 9)    0.00%    0.00%   58.74%    0.00%    4.87%    0.00%   36.39%
dirty_io_s(10)    0.00%    0.00%   57.07%    0.00%    5.15%    0.00%   37.78%
dirty_io_s(11)    0.00%    0.00%   52.86%    0.00%    4.72%    0.00%   42.42%
dirty_io_s(12)    0.00%    0.00%   52.42%    0.00%    4.63%    0.00%   42.95%
      poll( 0)    0.00%    0.53%    0.00%    0.00%    0.00%    0.00%   99.47%
 scheduler( 1)    2.40%    0.19%   31.46%    7.20%   18.40%    0.86%   39.50%
 scheduler( 2)    2.23%    0.18%   29.31%    6.87%   18.94%    0.83%   41.64%
 scheduler( 3)    2.12%    0.22%   30.32%    7.40%   19.26%    0.73%   39.96%
 scheduler( 4)    2.27%    0.19%   30.15%    7.50%   19.07%    0.78%   40.04%
 scheduler( 5)    2.34%    0.19%   31.03%    7.28%   19.27%    0.84%   39.03%
 scheduler( 6)    2.30%    0.17%   29.93%    7.42%   18.81%    0.68%   40.68%
 scheduler( 7)    2.45%    0.17%   29.16%    7.50%   18.91%    0.70%   41.10%
 scheduler( 8)    2.22%    0.18%   30.32%    6.87%   19.04%    0.66%   40.72%
 scheduler( 9)    2.28%    0.18%   29.19%    7.28%   18.82%    0.79%   41.47%
 scheduler(10)    2.28%    0.18%   28.56%    7.11%   19.17%    0.77%   41.94%
 scheduler(11)    2.24%    0.18%   30.12%    6.96%   18.89%    0.76%   40.85%
 scheduler(12)    2.31%    0.20%   29.76%    7.29%   18.94%    0.73%   40.77%
 scheduler(13)    2.38%    0.21%   30.18%    7.06%   19.19%    0.68%   40.30%
 scheduler(14)    2.25%    0.20%   30.32%    7.28%   19.09%    0.75%   40.11%
 scheduler(15)    2.34%    0.22%   30.58%    7.39%   18.81%    0.72%   39.93%
 scheduler(16)    2.28%    0.21%   29.44%    7.29%   19.11%    0.71%   40.97%
 scheduler(17)    2.30%    0.18%   29.20%    7.43%   18.52%    0.72%   41.65%
 scheduler(18)    2.04%    0.15%   23.48%    5.65%   16.95%    0.59%   51.14%
 scheduler(19)    0.00%    0.00%    0.00%    0.00%    0.02%    0.00%   99.98%
 scheduler(20)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
 scheduler(21)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
 scheduler(22)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
 scheduler(23)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
 scheduler(24)    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%

Stats per type:
         async    0.00%    0.00%    0.00%    0.00%    0.00%    0.00%  100.00%
           aux    4.08%    0.34%    0.00%    0.00%    0.11%    0.00%   95.47%
dirty_cpu_sche    0.00%    0.00%    0.02%    1.40%    0.07%    0.00%   98.51%
dirty_io_sched    0.00%    0.00%   53.86%    0.00%    4.77%    0.00%   41.37%
          poll    0.00%    0.53%    0.00%    0.00%    0.00%    0.00%   99.47%
     scheduler    1.71%    0.14%   22.19%    5.37%   14.13%    0.55%   55.91%
```",1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],1102995596,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
119,21400976759,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-21 15:46:48,1211221948,1825,,"From Slack ...

This indicates there is a backlog of activity in the log-structured merge tree.  You can tune the pause, but the pause is triggered because it is required ... the backend can't keep up with the volume of writes.

Can this happen without some resource being flat out?  I think it could do.  Each vnode leveled backend is made up of a number of processes, and for some operations these process must operate in series, and for some they can operate in parallel.  PUTs require just two processes to operate in series (the leveled_bookie and the leveled_inker), and they're doing very simple tasks.  Then in batches, work is pushed for the leveled_penciller, and that tries to do its work (shuffling keys/metadata into order within the LSM tree) in the background, in parallel to PUTs.  However, if it can't keep up there is a feedback mechanism that leads to the pause ... during the pause the leveled_bookie/leveled_inker can't take new PUTs but the leveled_penciller can continue its work in the background.  (note the pause doesn't happen right away, at first the bookie tries to just keep more state in memory before sending to the next batch, it only when the backlog persists that there is a pause).

The time it takes the leveled_penciller to do its background work is subject to the latency of each task that is required to be complete.  If resource utilisation is high that latency will go up, but even if resource utilisation is low the store could still get into a state whereby the time needed to complete the background task means that the penciller perpetually falls behind the pace of new writes.

In normal operation (outside of handoffs), because you should have 10-50 busy vnodes per node, resource utilisation slows everything down when things are busy ... the parallelism to maximise underlying resource comes through distribution of work across vnodes rather than having multiple processes within the vnode providing the parallelism necessary to push the resource.

Also in normal operation, there will be a load of GET activity ... and the way Riak/leveled is designed that GET activity has a cheap part and an expensive part, and the expensive part is handled only by the nodes that do the cheap part quickly ... so vnodes/nodes with backlogs end up doing less work.

Handoff is not normal.  There may only be PUTs occurring (so no GET races to handle load), and because of the transfer-limit not all vnodes are working and providing the parallelism to push load.  So it is possible to get vnodes that can't keep up, but without there being any given resource max'd up ... as within the vnode there is a an un-parallelised resource (the penciller) that can't keep pace with resource that is doing the immediate part of the PUT (the bookie and the inker).
The 10ms for the pause was a guess as to what would work.  I suspect that in your case it is not long enough, it should be increased to slow PUTs in this scenario and allow the penciller to keep up.  You probably need to set {riak_kv, backend_pause_ms}  to a higher value (perhaps 100ms or more) - but this might also need you to increase the timeout for handoffs.

The logs should help you here.  There are handoff logs that will tell you about timeouts, and also ""log_ref=P0024"" logs which tell you about penciller backlogs.

-----------

This problem area came up with another user recently.  They're seeing a cluster using full-sync from another cluster, and saw escalating memory usage, and when we investigated the high memory usage, at first it looked like a memory fragmentation issue within the BEAM.  However, the current hypothesis is that the penciller was getting behind, and so the leveled_bookie was building its in-memory buffer to compensate, but the penciller was never catching up, even with the pauses ... so memory was growing.  When the load finally stopped, the shadow of the expansion could still be seen as fragmentation.

Note that setting the riak_kv backend_pause_ms takes affect at startup, so you can't change it a run-time.  You need to change the value in riak.conf and restart

-----------

There's no obvious alternative solution, other than tuning this value.  It is hard to parallelise the activity in the leveled_penciller to use available resource to catch-up, as the use of a singular process in the current design eliminates a lot of potential race conditions ... and it is not clear that this is a problem in normal (non-handoff) operation with a mixture of GET/PUTs and all vnodes fired up and working in parallel.   Perhaps the backend pause should be adaptive i.e. if the backlog persists dynamically increase the pause.

-----------

Initially we have had this running on lots of clusters that have all managed transfers without problems.  However, every Riak store is different in terms of underlying hardware, volume of keys and size of objects.  This problem is much more likely to occur if: a) objects are generally small; b) vnodes have higher volumes of keys (running large stores on relatively low ring sizes).  Scenario a) reduces the work the inker/bookie has to do on PUTs so allows them to move faster.  Scenario b) increases the depth of the LSM tree requiring more merge work - creating more work for the penciller, making it move slower.

Clearly the potential for this to happen exists elsewhere, and the current feedback loop is confusing.  I will think about this further, especially with regards to a longer and more adaptive backend_pause timer.

",1628897,martinsumner,0,,[],[],1105401701,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
120,21416425528,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-22 11:17:49,1211221948,1825,,"There are a coupe of simple options here:

- Change the default backend pause, it was a guess, perhaps 10ms is just too short
- Change the multiplier [here](https://github.com/martinsumner/leveled/blob/086e06979e6f6d0d37b15df883c0ce2eb3c15892/src/leveled_bookie.erl#L2399) to a lower number, causing the bookie to pause sooner and more often

Also of interest, is the ordering with which merge files are chosen in the LSM tree when there is a backlog (it starts with the lowest, not the highest levels).  Are backlogs being exacerbated by having large L1, and hence meaning that L0 takes longer to merge into L1)?

Another configurable is to change the size of the bookie's ledger cache (which is already configurable).
",1628897,martinsumner,0,,[],[],1106413848,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
121,21597317705,IssuesEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-03 15:50:03,1224275889,1826,Scheduler Settings,As an offshoot of https://github.com/basho/riak_kv/issues/1820 - a thread related to testing different scheduler settings,1628897,martinsumner,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
122,21731446058,IssuesEvent,opened,30156044,pjaclark,791522,basho/riak_kv,176293,basho,2022-05-11 12:22:35,1232551635,1827,Cannot get Tictac AAE build status,"It would be useful to have some way to confirm that Tictac AAE has started/is building/has built it's trees. Certain admin operations (such as `aae_fold`) rely on the trees, but there is no way to know if they are fully available. At the moment, there is `riak-admin aae-status` for legacy AAE to show this, but nothing similar for Tictac AAE.  I propose something be made available under `riak-admin tictacaae-status` showing the status on the local node.",30156044,pjaclark,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
123,21959405134,IssuesEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-24 14:39:27,1246663725,1831,"Joins, 2i queries and bucket types","The testing of 2i  during cluster change operations was improved via this PR.  https://github.com/basho/riak_test/pull/1354/files

However, this test now fails intermittently at this point - https://github.com/basho/riak_test/blob/develop-3.0/tests/verify_2i_handoff.erl#L118

The cause of the failure is a worker crash, for a runner running the 2i query on a particular vnode.  When attempting to apply the coverage filter, it needs to chash the key .. but the key is in a typed bucket, and presumably the information about that typed bucket has not yet propagated after the join.  so the bucket properties is {error, no_type} not a list of properties, and attempting to find the key function in the bucket properties fails https://github.com/basho/riak_test/blob/develop-3.0/tests/verify_2i_handoff.erl#L118.

So this looks like this has always been an issue, you cannot reliably run 2i queries during joins (although potentially only impacting typed buckets).
",1628897,martinsumner,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
124,21959486482,IssuesEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-24 14:42:51,1211221948,1825,,,1628897,martinsumner,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
125,22061655703,IssuesEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-30 18:48:10,1051942480,1804,,,1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
126,22061659266,IssuesEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-30 18:48:31,1070577377,1807,,,1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
127,22061663090,IssuesEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-30 18:48:55,1161720703,1813,,,1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
128,22061665634,IssuesEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-30 18:49:10,1164153317,1815,,,1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
129,22061667338,IssuesEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-30 18:49:20,1164523730,1817,,,1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
130,22061670227,IssuesEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-30 18:49:36,1176654692,1819,,,1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
131,22061715429,IssuesEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-30 18:53:59,1193014932,1820,,,1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
132,22061717077,IssuesEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-30 18:54:08,1208273405,1824,,,1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
133,20621217716,IssuesEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-07 17:35:37,1161720703,1813,"Replication, delete_mode not keep and key amnesia","## Scenario

There are two clusters: ClusterA, ClusterB.  Both clusters are configured with the standard delete_mode (reaping on 3s timeout).

An object key (K1) belongs to a Preflist of {VnodeA1, VnodeA2, VnodeA3} on ClusterA, and {VnodeB1, VnodeB2, VnodeB3} on ClusterB.

An object is initiated with that key K1 on ClusterA (VnodeA1 say, acting as coordinator).  The creation is replicated.

The object is deleted on ClusterB (VnodeB1 say, acting as coordinator).  The deletion is replicated.

For some reason, on one cluster (ClusterA say) the reap does not occur on the timeout.

## Replication Goes Wrong

1. There is now an object on ClusterA, a tombstone with a vector clock like `[{VnodeA1, {1, TS1}}, {VnodeB1, {1, TS2}}]`, that does not exist on ClusterB.
2. nextgenrepl full-sync now runs with ClusterA as the src, and ClusterB as the snk.
3. The aae_exchange discovers that A > B for K1 as K1 not_found on B.  It puts a reference on the replication queue.
4. The fetch, which is run via riak_kv_get_fsm pulls across the tombstone object.  But as it prompts a read (via riak_kv_get_fsm), the read final action spots that this is a tombstone, and so triggers a maybe_delete, which causes the tombstone to be reaped from ClusterA.
5. The push of the tombstone to ClusterB, is performed as expected on VnodeB2 and VnodeB3.  However on VnodeB1 there is [key amnesia](https://github.com/basho/riak_kv/pull/1643) - this vnode previously coordinated the write, but has no backend memory because of the reap.
6. Due to key amnesia, the object is written with an updated vector clock.
7. The riak_client:push function which has replicated the object prompts a GET of the object after the `asis` PUT.  This GET is intended to prompt any delete actions (e.g. it is expected to reap).   However, in this case it prompts a read-repair instead (there can only be one final action), because the tombstone's vclock differs at VnodeB1 from VnodeB2 and VnodeB3 due to the new actor epoch generated by key amnesia.
8. This means that the object has been reaped from ClusterA, but exists on ClusterB with a vector clock following read repair like `[{VnodeA1, {1, TS1}}, {VnodeB1, {1, TS2}}, {VnodeB1.1, {1, TS3}}]`.
9. Now if full-sync runs from B to A - the same thing occurs in reverse.  The net effect is the tombstone is reaped from ClusterB, but re-added to ClusterA with an expanded vector clock due to the new actor epoch (triggered by the object update history from VnodeA1) e.g.: `[{VnodeA1, {1, TS1}}, {VnodeB1, {1, TS2}}, {VnodeB1.1, {1, TS3}}, {VnodeA1.1, {1, TS4}}]`

... this object can then loop around indefinitely, forever increasing the size of the vector clock.



",1628897,martinsumner,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
134,20668975429,IssuesEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-09 22:48:01,1164523730,1817,Real-time replication - switch to `riak_kv_overflow_queue`,"The `riak_kv_overflow_queue` has been added as part of [this enhancement to the reaper/eraser queues](https://github.com/basho/riak_kv/pull/1809).

Currently the realtime replication queue is in memory, and of a limited fix size.  Change this to use the overflow queue, to allow for:

- larger backlogs;
- backlogs to persist across reboots.

[The reason](https://en.wikipedia.org/wiki/G._K._Chesterton#Chesterton's_fence) why originally this was simply an in-memory queue, was that full-sync will always fill the gaps.  Although full-sync is now cheaper than key-listing, especially when gaps are small, it is still a slow operation.

If there are o(100K) or greater gaps, full-sync is going to be more time consuming and expensive than draining from a queue, if such a queue exists.

In implementing this, the queue should persist across backlogs but start empty.  There should be an admin command to empty a previous real-time queue on the node, so that replaying such a persisted backlog is an operator choice.

",1628897,martinsumner,1628897,martinsumner,['martinsumner'],[1628897],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
135,20870622488,IssuesEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-22 11:35:27,1176654692,1819,"Tombstone pause, reap pause","The `riak_kv_delete` process has a `tombstone_pause`(default 10ms) added as part of a [general set of improvements in the TTAAE full-sync process](https://github.com/basho/riak_kv/pull/1776).

This pause occurs between the delete (the addition of the tombstone), and the GET that prompts the reap.  It was found in timeout based `delete_mode`, the race between the PUT of the tomb and the GET could result in tombstones lingering, and then those tombstones caused issues in full-sync.

The root cause of the tombstones causing full-sync problems has now been resolved: https://github.com/basho/riak_kv/issues/1813.  Therefore the necessity of this pause is no longer clear.  Volume erases have been tested with this reduced to 2ms with no issues.

Clearing this to 0, would though reduce any constraint on the performance of the eraser.  There remains some benefits in the eraser being a background process with some (albeit accidental rate-limiting).

The proposal is to reduce this pause from 10ms to 2s.  Further, the same pause should be applied in the reaper, so that riak_kv_reaper works at a consistent pace to the riak_kv_eraser.",1628897,martinsumner,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
136,25127906021,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-11-09 13:10:22,1442034005,1835,,"Is this specifically about setting in bucket types?  I know there is a test for the 'PR' value here, but it is passed in as a GET option - https://github.com/basho/riak_test/blob/develop-3.0/tests/pr_pw.erl

In theory the bucket properties should be checked [here](https://github.com/basho/riak_kv/blob/develop-3.0/src/riak_kv_get_fsm.erl#L316), and then that is added to the [get_core](https://github.com/basho/riak_kv/blob/develop-3.0/src/riak_kv_get_fsm.erl#L342-L346).  The [get_core checks](https://github.com/basho/riak_kv/blob/develop-3.0/src/riak_kv_get_core.erl#L218), and only gives a positive response [when satisfied](https://github.com/basho/riak_kv/blob/develop-3.0/src/riak_kv_get_core.erl#L253). ",1583029,systream,0,,[],[],1308728638,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
137,25128151565,IssueCommentEvent,created,1583029,systream,791522,basho/riak_kv,176293,basho,2022-11-09 13:20:18,1442034005,1835,,"The issue is on line 313 in riak_kv_get_fsm. The `DEFAULT_PR` is `0` instead of default. 

To reproduce: have 3 riak nodes. 
Test bucket properties: 

```
test_pw is active

young_vclock: 20
w: 3
sync_on_write: backend
small_vclock: 50
rw: quorum
r: 3
pw: 3
precommit: []
pr: 3
postcommit: []
old_vclock: 86400
notfound_ok: true
node_confirms: 0
n_val: 3
linkfun: {modfun,riak_kv_wm_link_walker,mapreduce_linkfun}
last_write_wins: false
dw: quorum
dvv_enabled: true
chash_keyfun: {riak_core_util,chash_std_keyfun}
big_vclock: 50
basic_quorum: false
allow_mult: true
active: true
claimant: 'dev1@127.0.0.1'
```

Stop one riak node.

Do a get: 
```
 riakc_pb_socket:get(poolboy:checkout(riakc_pool), {<<""test_pw"">>, <<""test"">>}, <<""fooo1"">>).
{error,notfound}
```
```
riakc_pb_socket:get(poolboy:checkout(riakc_pool), {<<""test_pw"">>, <<""test"">>}, <<""fooo1"">>, [{pr, 3}]).
{error,<<""{pr_val_unsatisfied,3,2}"">>}
```


",1583029,systream,0,,[],[],1308744326,1583029,systream,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
138,25129478850,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-11-09 14:12:12,1442034005,1835,,"Thanks, I see it now.

I've done a quick check of the PUT fsm as well as the GET fsm, and it looks like it is only this value where the error exists.

I'm testing some leveled changes at the moment which may require a 3.0.12 release, so I will bundle a fix for this in as well.  Target is to have 3.0.12 with this fix by the end of the week.

",1583029,systream,0,,[],[],1308825800,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
139,25131546865,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-11-09 15:28:43,1442034005,1835,,"If you have time, that would help a great deal.",1583029,systream,0,,[],[],1308935799,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
140,25351215935,IssueCommentEvent,created,89027847,efcasado-kivra,791522,basho/riak_kv,176293,basho,2022-11-19 19:58:11,1456609355,1837,,"If we manage to get this deployed to our production cluster, we may be able to share some logs that may help with the #1707 investigation.",603610,efcasado,0,,[],[],1320957959,89027847,efcasado-kivra,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
141,25358829310,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-11-20 17:03:32,1456609355,1837,,"Good spot, and thank-you for the contribution.  I'm hoping to make a 3.0.12 release (including this fix) available in the next ten days.  ",603610,efcasado,0,,[],[],1321186036,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
142,25456006591,IssueCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-11-24 14:14:49,1443506535,1836,,"https://github.com/basho/riak_test/pull/1366

Test added to confirm failure, and success of PR",1583029,systream,0,,[],[],1326509980,1628897,martinsumner,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
143,24532293379,PullRequestEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-10-11 17:29:52,1083629155,1833,Mas i379 snapshottime,Update tag for leveled change,1628897,martinsumner,0,,[],[],0,0,,,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i379-snapshottime,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
144,24532507663,PullRequestEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-10-11 17:40:59,1083629155,1833,,,1628897,martinsumner,0,,[],[],0,0,,6ebac46bf9ead05dae716b05b2a3f5767c5288db,1628897,martinsumner,0,,develop-3.0,791522,basho/riak_kv,mas-i379-snapshottime,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
145,24668579195,PullRequestEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-10-18 12:14:35,1090700594,1834,Update erlang.yml,Add cmake to allow build of snappy,1628897,martinsumner,0,,[],[],0,0,,,0,,0,,develop,791522,basho/riak_kv,mas-cibuild-cmake,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
146,24669379598,PullRequestEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-10-18 12:46:38,1090700594,1834,,,1628897,martinsumner,0,,[],[],0,0,,5fc3808f6cf0b692e399833027c09a80511adc4d,1628897,martinsumner,0,,develop,791522,basho/riak_kv,mas-cibuild-cmake,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
147,21119025603,IssuesEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-05 11:27:00,1193014932,1820,Memory fragmentation,"There have been reports from users of Riak suffering memory fragmentation with large clusters with leveled backends.

This appears to be an issue with both OTP 22.3 as well as OTP R16basho.

Primary indicator is:

`recon_alloc:memory(usage).`

Which can be between 50% and 80%.

This is not universal, some leveled/Riak clusters do not see this.  However, on clusters that do see it there is limited mitigation other than restarting nodes.  The excess memory takes from the Page Cache, and so can have a direct impact on performance.  In extreme cases unexpected and sudden memory exhaustion has been observed.

Horizontal scaling currently appears to offer better mitigation than vertical scaling.

The fragmentation is primarily with the `eheap_alloc` and `binary_alloc` and associated with inefficient use of multiblock carriers, primarily these carriers not seeming to ever transition to empty and be reclaimed.",1628897,martinsumner,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
148,21230647035,IssuesEvent,opened,5654175,maciejbiesek,791522,basho/riak_kv,176293,basho,2022-04-11 20:09:51,1200477764,1821,Unable to download,"I would like to download and run RIAK KV, but every link I'm checking has an error message PermanentRedirect with further info: ""The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.""",5654175,maciejbiesek,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
149,21353382210,IssuesEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-19 12:16:48,1208273405,1824,Corrupted Object and AAE,"If an object is corrupted in a store - the AAE folds will keep crashing and the AAE trees will never be rebuilt.

It isn't possible to identify which object the fold crashes on, and there is no automated way of doing anything about the crash.

@s2hc-johan",1628897,martinsumner,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
150,21400924097,IssuesEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-21 15:44:07,1211221948,1825,Leveled and backend pause,"When the leveled backend has a backlog of activity it triggers a pause, to try and delay to give it time to process the backlog.

It seems in PUT only operations, such as when seeding a cluster via range_repl or full-sync, or in transfers; that this might be a more common issue than expected, and that the current default pause may be insufficient.

The pause should probably be adaptive.",1628897,martinsumner,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
151,25127314537,IssuesEvent,opened,1583029,systream,791522,basho/riak_kv,176293,basho,2022-11-09 12:45:37,1442034005,1835,Ignoring pr value from bucket type. ,"PR value set in bucket type ignored when reading data. 

We set PR to 4 but unfortunately it is ignored by get_fsm. 
https://github.com/basho/riak_kv/blob/develop-3.0/src/riak_kv_get_fsm.erl#L313

It caused data loss, because we got data from non primary vnodes what leads us to not found, we created a new object and overwrite it. 
",1583029,systream,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
152,24532283859,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-10-11 17:29:22,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,11295347464,e48ab79c55c3168280f0e700b863577fff9e2ad3,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Update rebar.config'],0,0,,,,0,,,0,,,[]
153,24532507759,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-10-11 17:40:59,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,11295459199,6ebac46bf9ead05dae716b05b2a3f5767c5288db,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Mas i379 snapshottime (#1833)\n\n* Update rebar.config\r\n\r\n* Update rebar.config'],0,0,,,,0,,,0,,,[]
154,24666853640,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-10-18 10:57:34,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,11365726461,b11173f80b29e5fed17886ceaa29a6f52c51ccf1,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['Update to mainstream redbug\n\nPR now accepted into redbug, so return to master.  Changes to pass eqc tests on OTP 24.  Also all eunit tests passing on OTP 25.1.1']",0,0,,,,0,,,0,,,[]
155,24669379840,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-10-18 12:46:38,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,11366975473,5fc3808f6cf0b692e399833027c09a80511adc4d,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Update erlang.yml (#1834)\n\nAdd cmake to allow build of snappy'],0,0,,,,0,,,0,,,[]
156,20129125294,PullRequestEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-02-08 12:18:10,842746567,1810,Develop eqc otp24,,1628897,martinsumner,0,,[],[],0,0,,,0,,0,,develop,791522,basho/riak_kv,develop-eqc-otp24,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
157,20132271648,PullRequestEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-02-08 14:51:10,842900838,1811,Change time out checking in unit test,"Previously this relied on catching a reply to a sync_send_event which had timed out through a receive.  This appears not to work now - a receive for the same PID as the crashed send, will not receive a response.  It is assumed this is related to smarter references for sync replies in OTP 24.

A more natural way is used now (albeit one that is dependent on timers).  We allow the sync request to get a delayed response, and use a timer to confirm it is delayed and not immediate.",1628897,martinsumner,0,,[],[],0,0,,,0,,0,,develop,791522,basho/riak_kv,mas-mrc-eunitfix,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
158,20560794654,PullRequestEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-03 11:41:40,870501164,1812,Mas i1804 peerdiscovery,"See https://github.com/basho/riak_kv/issues/1804

The heart of the problem is how to avoid needing configuration changes on sink clusters when source clusters are bing changed.  This allows for new nodes to be discovered automatically, from configured nodes.

Default behaviour is to always fallback to configured behaviour.

Worker Counts and Per Peer Limits need to be set based on an understanding of whether this will be enabled.  Although, if per peer limit is left to default, the consequence will be the worker count will be evenly distributed (independently by each node).  Note, if Worker Count mod (Src Node Count) =/= 0 - then there will be no balancing of the excess workers across the sink nodes. ",1628897,martinsumner,0,,[],[],0,0,,,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
159,20644963752,PullRequestEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-08 20:07:39,874544603,1814,Allow for a dual final action - repair and delete,"If there is repair only (as a priority over delete - which really means trigger reap if required), then there are scenarios where tombstones can perpetually re-appear, especially were there is key amnesia.",1628897,martinsumner,0,,[],[],0,0,,,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1813-getfinalaction,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
160,20668677512,PullRequestEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-09 22:25:13,875789740,1816,Mas i1815 autocheck,"To simplify the configuration, rather than have the operator select `all_check` `day_check` `range_check` etc, there is now a default strategy of `auto_check` which tries to do a sensible thing:

- do `range_check` when a range is set, otherwise
- do `all_check` if it is out of hours (in the all_check window), otherwise
- do `day_check`

Some stats added to help with monitoring, the detail is still also in the console logs.",1628897,martinsumner,0,,[],[],0,0,,,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1815-autocheck,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
161,20739350247,PullRequestEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-14 16:16:42,879259134,1818,Mas i1817 overflowrtq,"Expand on use of `riak_kv_overflow_queue` so that it is used by the riak_kv_replrtq_src, as well as riak_kv_reaper and riak_kv_eraser.

This means that larger queue sizes can be supported for `riak_kv_replrtq_src` without having to worry about compromising the memory of the node.  This should allow for `repl_keys_range` AAE folds to generate very large replication sets, without clogging the node worker pool by pausing so that real-time replication can keep up.

the overflow queues are deleted on shutdown (if there is a queue on disk).  The feature is to allow for larger queues without memory exhaustion, persistence is not used to persist queues across restarts.",1628897,martinsumner,0,,[],[],0,0,,,0,,0,,mas-i1807-overflowqueues,791522,basho/riak_kv,mas-i1817-overflowrtq,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
162,21728885547,PullRequestEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-11 10:07:36,799173754,1809,,,1628897,martinsumner,0,,[],[],0,0,,2979109afef7c0b12bcb457210b7ab0d7e677236,1628897,martinsumner,0,,develop-3.0,791522,basho/riak_kv,mas-i1807-overflowqueues,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
163,21732835875,PullRequestEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-11 13:27:07,870501164,1812,,,1628897,martinsumner,0,,[],[],0,0,,aeca1cab68de678909fe4a81ec9456ca4d93a1a4,1628897,martinsumner,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
164,21757858595,PullRequestEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 15:40:46,934999390,1828,Mas i1807 overflowqueues,"Expand on use of riak_kv_overflow_queue so that it is used by the riak_kv_replrtq_src, as well as riak_kv_reaper and riak_kv_eraser.

This means that larger queue sizes can be supported for riak_kv_replrtq_src without having to worry about compromising the memory of the node. This should allow for repl_keys_range AAE folds to generate very large replication sets, without clogging the node worker pool by pausing so that real-time replication can keep up.

The overflow queues are deleted on shutdown (if there is a queue on disk). The feature is to allow for larger queues without memory exhaustion, persistence is not used to persist queues across restarts.",1628897,martinsumner,0,,[],[],0,0,,,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1807-overflowqueues,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
165,21758073471,PullRequestEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 15:51:29,934999390,1828,,,1628897,martinsumner,0,,[],[],0,0,,,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1807-overflowqueues,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
166,21758377578,PullRequestEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 16:06:51,879259134,1818,,,1628897,martinsumner,0,,[],[],0,0,,273e0c25c123628f2c382f4526eb9f16341674ee,0,,0,,mas-i1807-overflowqueues,791522,basho/riak_kv,mas-i1817-overflowrtq,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
167,21758403230,PullRequestEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 16:08:11,935027731,1829,Mas i1817 overflowrtq,"Expand on use of riak_kv_overflow_queue so that it is used by the riak_kv_replrtq_src, as well as riak_kv_reaper and riak_kv_eraser.

This means that larger queue sizes can be supported for riak_kv_replrtq_src without having to worry about compromising the memory of the node. This should allow for repl_keys_range AAE folds to generate very large replication sets, without clogging the node worker pool by pausing so that real-time replication can keep up.

The overflow queues are deleted on shutdown (if there is a queue on disk). The feature is to allow for larger queues without memory exhaustion, persistence is not used to persist queues across restarts.",1628897,martinsumner,0,,[],[],0,0,,,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1817-overflowrtq,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
168,21759972996,PullRequestEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 17:39:07,935027731,1829,,,1628897,martinsumner,0,,[],[],0,0,,3d2d4e232000b0a48254788fe0aadad25e4f547a,1628897,martinsumner,0,,develop-3.0,791522,basho/riak_kv,mas-i1817-overflowrtq,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
169,21761586638,PullRequestEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 19:20:56,794726811,1808,,,1628897,martinsumner,0,,[],[],0,0,,64d11adfa4596eb1997ea96024e4ca8be64edb25,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1807-boundqueues,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
170,21774958323,PullRequestEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-13 12:58:27,935148191,1830,,,1628897,martinsumner,0,,[],[],0,0,,71923c0dfc10cadfc45e0e31e2131201426338da,1628897,martinsumner,0,,develop-3.0,791522,basho/riak_kv,mas-i1813-keepamnesia,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
171,24532527501,ReleaseEvent,published,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-10-11 17:42:01,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,79561751,riak_kv-3.0.11,Riak KV 3.0.11 - Release,1628897,martinsumner,Updated tag for leveled change,[]
172,21350293156,PullRequestEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-19 09:24:51,912849124,1823,Mas i1813 keepamnesia,,1628897,martinsumner,0,,[],[],0,0,,,0,,0,,mas-i1807-overflowqueues,791522,basho/riak_kv,mas-i1813-keepamnesia,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
173,20129616145,PushEvent,added,3169010,martincox,791522,basho/riak_kv,176293,basho,2022-02-08 12:46:03,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9040502267,a1e2c40d8c83161e7e7fe61c354021543a3f94bb,['Martin Cox'],['martin@cox.dev'],['Get rid of junk debug messages for failing test.'],0,0,,,,0,,,0,,,[]
174,20130291659,PushEvent,added,3169010,martincox,791522,basho/riak_kv,176293,basho,2022-02-08 13:19:07,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9040867067,05a4aa74f0b1a7251780c00b8e2d04c1eacd9c07,"['Martin Sumner', 'Martin Sumner', 'Martin Cox']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin@cox.dev']","['EQC fixes\n\nBucket property validation was broken by erroneous changes to clause matches in riak_kv_bucket.\n\nput_fsm_eqc required removal of lager references', 'Update riak_kv_wm_utils.erl\n\nMap string-based URI scheme to atoms for compatibility with webmachine.\n\nuri_string:parse returns outputs as unicode:chardata() compatible with https://www.ietf.org/rfc/rfc3986.txt - but the only relevant of these strings (see https://en.wikipedia.org/wiki/List_of_URI_schemes) are http and https.\n\nAs per RFC both lower and upper case supported.', 'Merge pull request #1810 from basho/develop-eqc-otp24\n\nDevelop eqc otp24']",0,0,,,,0,,,0,,,[]
175,20133924767,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-02-08 16:07:40,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9042631218,1d05b187136472d89406e70b568e68e5b8583fe4,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Clarify comments'],0,0,,,,0,,,0,,,[]
176,20152846776,PushEvent,added,3169010,martincox,791522,basho/riak_kv,176293,basho,2022-02-09 14:02:35,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9052062000,41ee8929b34faf705290fc53d10273e7d2852b40,"['Martin Sumner', 'Martin Sumner', 'Martin Cox']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin@cox.dev']","['Change time out checking in unit test\n\nPreviously this relied on catching a reply to a sync_send_event which had timed out through a receive.  This appears not to work now - a receive for the same PID as the crashed send, will not receive a response.  It is assumed this is related to smarter references for sync replies in OTP 24.\n\nA more natural way is used now (albeit one that is dependent on timers).  We allow the sync request to get a delayed response, and use a timer to confirm it is delayed and not immediate.', 'Clarify comments', 'Merge pull request #1811 from basho/mas-mrc-eunitfix\n\nChange time out checking in unit test']",0,0,,,,0,,,0,,,[]
177,20332394195,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-02-18 11:54:55,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9139229110,2830e96447a56aea117965e9f95e51fa2f5316ab,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Consistently name http client\n\nRequired for OTP 25'],0,0,,,,0,,,0,,,[]
178,24371801409,WatchEvent,started,42013376,ThatoK97,791522,basho/riak_kv,176293,basho,2022-10-03 20:00:18,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
179,20888695453,PullRequestReviewCommentEvent,created,638015,ThomasArts,791522,basho/riak_kv,176293,basho,2022-03-23 08:46:40,870501164,1812,,Do you really want 60 hardcoded here or rather use the macro `?DISCOVERY_TIMEOUT_SECONDS`.,1628897,martinsumner,0,,[],[],0,0,,ae6acae02211d22deb3a42407322df600f98364e,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918398367,832999986,src/riak_kv_replrtq_peer.erl,638015,ThomasArts,0,,[],[],[],0,0,,,,0,,,0,,,[]
180,20888787146,PullRequestReviewCommentEvent,created,638015,ThomasArts,791522,basho/riak_kv,176293,basho,2022-03-23 08:51:50,870501164,1812,,"It takes a few moments to realize that there are 2 interfaces for `prompt discovery` where this one goes via `handle_info` and has only one argument that does a lookup for the PeerInfo at the time it executes, whereas the cast version gets the PeerInfo in the interface.


Probably one should comment this difference more clearly.",1628897,martinsumner,0,,[],[],0,0,,ae6acae02211d22deb3a42407322df600f98364e,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918404917,833004509,src/riak_kv_replrtq_peer.erl,638015,ThomasArts,0,,[],[],[],0,0,,,,0,,,0,,,[]
181,20888830261,PullRequestReviewCommentEvent,created,638015,ThomasArts,791522,basho/riak_kv,176293,basho,2022-03-23 08:54:14,870501164,1812,,"I kind of dislike this... you leave the context of the genserver in the cast to later return to it... I wonder if it would not be better to actually call `prompt_discovery(QueueName, PeerInfo, regular)` at this point.",1628897,martinsumner,0,,[],[],0,0,,ae6acae02211d22deb3a42407322df600f98364e,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918407923,833006663,src/riak_kv_replrtq_peer.erl,638015,ThomasArts,0,,[],[],[],0,0,,,,0,,,0,,,[]
182,20889228756,PullRequestReviewCommentEvent,created,638015,ThomasArts,791522,basho/riak_kv,176293,basho,2022-03-23 09:15:37,870501164,1812,,"I guess this is by design, but if you run with Type `count_change` then CurrentPeers is the empty list. So in case discover_peers returns an empty list, you don't know in which of the two cases you are.

Both cases return false, but the side effects are different.",1628897,martinsumner,0,,[],[],0,0,,ae6acae02211d22deb3a42407322df600f98364e,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918436366,833026655,src/riak_kv_replrtq_peer.erl,638015,ThomasArts,0,,[],[],[],0,0,,,,0,,,0,,,[]
183,20889377874,PullRequestReviewCommentEvent,created,638015,ThomasArts,791522,basho/riak_kv,176293,basho,2022-03-23 09:23:33,870501164,1812,,Personally I find the code clearer if you inline this function.,1628897,martinsumner,0,,[],[],0,0,,ae6acae02211d22deb3a42407322df600f98364e,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918446834,833034190,src/riak_client.erl,638015,ThomasArts,0,,[],[],[],0,0,,,,0,,,0,,,[]
184,20889472473,PullRequestReviewCommentEvent,created,638015,ThomasArts,791522,basho/riak_kv,176293,basho,2022-03-23 09:28:23,870501164,1812,,"This terminate function may now take considerably more time.

If the supervisor terminates this server, how likely is it that this takes too long compared to the time you want it restarted?",1628897,martinsumner,0,,[],[],0,0,,ae6acae02211d22deb3a42407322df600f98364e,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918453410,833038769,src/riak_kv_replrtq_snk.erl,638015,ThomasArts,0,,[],[],[],0,0,,,,0,,,0,,,[]
185,20889502984,PullRequestReviewCommentEvent,created,638015,ThomasArts,791522,basho/riak_kv,176293,basho,2022-03-23 09:29:53,870501164,1812,,Why using app_helper for getting and application for setting? Is this an artefcat of OTP18 or so?,1628897,martinsumner,0,,[],[],0,0,,ae6acae02211d22deb3a42407322df600f98364e,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918455530,833040183,src/riak_kv_replrtq_snk.erl,638015,ThomasArts,0,,[],[],[],0,0,,,,0,,,0,,,[]
186,20890047363,PullRequestReviewCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-23 09:58:04,870501164,1812,,"Never thought about this much, just following inline with its use elsewhere.  Looking at the docs application:get_env/3 didn't exist until R16B ... and Riak was initially written prior to that ... so I guess it is just a legacy of this.

It might be one thing to add to the list for the OTP 24+ version of Riak, to refactor throughout and to take this out.",1628897,martinsumner,0,,[],[],0,0,,ae6acae02211d22deb3a42407322df600f98364e,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918497595,833068090,src/riak_kv_replrtq_snk.erl,1628897,martinsumner,0,,[],[],[],0,0,,,,0,,,0,,,[]
187,20891655857,PullRequestReviewCommentEvent,created,1933698,s2hc-johan,791522,basho/riak_kv,176293,basho,2022-03-23 11:02:49,875789740,1816,,"I think that the documentation refers to 24 still, don’t know which is correct but they should probably be conssitent",1628897,martinsumner,0,,[],[],0,0,,67adba8bd2def59b0699775c496d34041029bb63,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1815-autocheck,918568275,833128594,priv/riak_kv.schema,1933698,s2hc-johan,0,,[],[],[],0,0,,,,0,,,0,,,[]
188,20891655864,PullRequestReviewCommentEvent,created,1933698,s2hc-johan,791522,basho/riak_kv,176293,basho,2022-03-23 11:04:07,875789740,1816,,revious -> previous,1628897,martinsumner,0,,[],[],0,0,,67adba8bd2def59b0699775c496d34041029bb63,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1815-autocheck,918568275,833129588,priv/riak_kv.schema,1933698,s2hc-johan,0,,[],[],[],0,0,,,,0,,,0,,,[]
189,20891655869,PullRequestReviewCommentEvent,created,1933698,s2hc-johan,791522,basho/riak_kv,176293,basho,2022-03-23 11:07:02,875789740,1816,,should it be “cluster sof” -> “clusters of”,1628897,martinsumner,0,,[],[],0,0,,67adba8bd2def59b0699775c496d34041029bb63,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1815-autocheck,918568275,833132014,priv/riak_kv.schema,1933698,s2hc-johan,0,,[],[],[],0,0,,,,0,,,0,,,[]
190,20891655947,PullRequestReviewCommentEvent,created,1933698,s2hc-johan,791522,basho/riak_kv,176293,basho,2022-03-23 11:12:14,875789740,1816,,"Not looking deep, do we have tests that cover functionality or should I script to look for spelling mistakes or something?",1628897,martinsumner,0,,[],[],0,0,,67adba8bd2def59b0699775c496d34041029bb63,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1815-autocheck,918568275,833136861,src/riak_kv_stat.erl,1933698,s2hc-johan,0,,[],[],[],0,0,,,,0,,,0,,,[]
191,20891655961,PullRequestReviewCommentEvent,created,1933698,s2hc-johan,791522,basho/riak_kv,176293,basho,2022-03-23 11:16:10,875789740,1816,,don’t really know what to look for here,1628897,martinsumner,0,,[],[],0,0,,67adba8bd2def59b0699775c496d34041029bb63,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1815-autocheck,918568275,833140017,src/riak_kv_stat.erl,1933698,s2hc-johan,0,,[],[],[],0,0,,,,0,,,0,,,[]
192,20891655941,PullRequestReviewCommentEvent,created,1933698,s2hc-johan,791522,basho/riak_kv,176293,basho,2022-03-23 11:18:14,875789740,1816,,default sync with docs,1628897,martinsumner,0,,[],[],0,0,,67adba8bd2def59b0699775c496d34041029bb63,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1815-autocheck,918568275,833141623,priv/riak_kv.schema,1933698,s2hc-johan,0,,[],[],[],0,0,,,,0,,,0,,,[]
193,20891655943,PullRequestReviewCommentEvent,created,1933698,s2hc-johan,791522,basho/riak_kv,176293,basho,2022-03-23 11:18:32,875789740,1816,,default sync with docs,1628897,martinsumner,0,,[],[],0,0,,67adba8bd2def59b0699775c496d34041029bb63,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1815-autocheck,918568275,833141850,priv/riak_kv.schema,1933698,s2hc-johan,0,,[],[],[],0,0,,,,0,,,0,,,[]
194,20891631908,PullRequestReviewCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-23 11:21:38,870501164,1812,,"Tried to avoid this confusion by not relying on the empty list mis-match - rather a specific mis-match between list and atom.  Commented as well.  Don't think it is super-clean still, but improved maybe.",1628897,martinsumner,0,,[],[],0,0,,ecd3ea830f6a3352fa2e7a9e9ecd3a2c53b2928f,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918605094,833144307,src/riak_kv_replrtq_peer.erl,1628897,martinsumner,0,,[],[],[],0,0,,,,0,,,0,,,[]
195,20892146249,PullRequestReviewCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-23 11:50:49,875789740,1816,,"The riak_test test checks the stats .  If you look at the get_stats function and its use in https://github.com/basho/riak_test/blob/mas-i1804-peerdiscovery/tests/nextgenrepl_rtq_peerdiscovery.erl.

There is also a verify_riak_stats test which gets updated, which just checks that all things are named correctly, and there are no unexpected entries:  https://github.com/basho/riak_test/blob/mas-i1804-peerdiscovery/tests/verify_riak_stats.erl

If this looks like it is covering everything OK, then we're fine.  However, if anything jumps out as looking weird, flag it, and I will add some coverage to the test.",1628897,martinsumner,0,,[],[],0,0,,67adba8bd2def59b0699775c496d34041029bb63,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1815-autocheck,918638610,833168335,src/riak_kv_stat.erl,1628897,martinsumner,0,,[],[],[],0,0,,,,0,,,0,,,[]
196,21016095317,PullRequestReviewCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-30 11:09:23,875789740,1816,,"Yes.  The configuration is based on the bitcask merge window, where you can set a window out of hours for merges to happen.  In this case we can set a window out of hours for all_checks  to work in.  So if you have delta in historic data, it will be resolved out of hours.",1628897,martinsumner,0,,[],[],0,0,,67adba8bd2def59b0699775c496d34041029bb63,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1815-autocheck,925940759,838417360,docs/NextGenREPL.md,1628897,martinsumner,0,,[],[],[],0,0,,,,0,,,0,,,[]
197,21759637743,PullRequestReviewCommentEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 17:18:44,935027731,1829,,"There are separate locations for each queue, which you can't really see from the diff:

```
%% @doc A path under which the eraser overload queue will be stored.
{mapping, ""eraser_dataroot"", ""riak_kv.eraser_dataroot"", [
  {default, ""$(platform_data_dir)/kv_eraser""},
  {datatype, directory}
]}.

%% @doc A path under which the reaper overload queue will be stored.
{mapping, ""reaper_dataroot"", ""riak_kv.reaper_dataroot"", [
  {default, ""$(platform_data_dir)/kv_reaper""},
  {datatype, directory}
]}.

%% @doc A path under which the reader overload queue will be stored.
{mapping, ""reader_dataroot"", ""riak_kv.reader_dataroot"", [
  {default, ""$(platform_data_dir)/kv_reader""},
  {datatype, directory}
]}.

%% @doc A path under which the repl real-time overload queue will be stored.
{mapping, ""replrtq_dataroot"", ""riak_kv.replrtq_dataroot"", [
  {default, ""$(platform_data_dir)/kv_replrtqsrc""},
  {datatype, directory}
]}.
```

",1628897,martinsumner,0,,[],[],0,0,,dbd20f3316c966a2a1568a2c61fd7c90f5195f04,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1817-overflowrtq,971212584,871634542,priv/riak_kv.schema,1628897,martinsumner,0,,[],[],[],0,0,,,,0,,,0,,,[]
198,25149589717,PullRequestEvent,opened,1583029,systream,791522,basho/riak_kv,176293,basho,2022-11-10 08:59:37,1117355782,1836,Change the default PR value from 0 to default,"Fix for #1835. 
",1583029,systream,0,,[],[],0,0,,,0,,0,,develop-3.0,424169838,systream/riak_kv,develop-3.0,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
199,25350592854,PullRequestEvent,opened,603610,efcasado,791522,basho/riak_kv,176293,basho,2022-11-19 18:20:22,1128904867,1837,[WIP] Fix issue that crashes vnode worker when object contents is empty,,603610,efcasado,0,,[],[],0,0,,,0,,0,,develop-3.0,568177985,efcasado/riak_kv,efcasado-riak_object-is_head-function_clause-error,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
200,25438546437,PullRequestEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-11-23 19:51:26,1134011900,1838,Mas i1121 reip3,"Add a `reip_manual/1` function to riak_kv_console that allows reip to be done with knowledge of ring_sir and cluster name - avoiding the need to load riak_core.

https://github.com/basho/riak_core/pull/992

https://github.com/basho/riak/issues/1121",1628897,martinsumner,0,,[],[],0,0,,,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1121-reip3,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
201,25552219892,PullRequestEvent,opened,1107079,hmmr,791522,basho/riak_kv,176293,basho,2022-11-30 01:22:53,1139726647,1839,add missing function clause repair_keys_range in convert_fold,"This is to unbreak `aae_fold` for `repair_keys_range`.

In collaboration with @pjaclark.",1107079,hmmr,0,,[],[],0,0,,,0,,0,,develop-3.0,378946268,hmmr/riak_kv,patch-1,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
202,25592031320,PullRequestEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-01 13:41:25,1117355782,1836,,,1583029,systream,0,,[],[],0,0,,bb5b610a0de865886312528893c4b990bc1517fd,1628897,martinsumner,0,,develop-3.0,424169838,systream/riak_kv,develop-3.0,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
203,25698096973,PullRequestEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-06 18:00:55,1134011900,1838,,,1628897,martinsumner,0,,[],[],0,0,,159df407282e67b57cb7a43f36ed375324fcb035,1628897,martinsumner,0,,develop-3.0,791522,basho/riak_kv,mas-i1121-reip3,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
204,25916559914,PullRequestEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-15 17:51:35,1167163410,1840,Update log,"Make situation clearer in log

Rather than change behaviour, just make it clear that the warning can be ignored when shutting down.  This should avoid unnecessary concern.",1628897,martinsumner,0,,[],[],0,0,,,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1824-crashfold,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
205,25925945864,PullRequestEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-16 04:28:43,1167163410,1840,,,1628897,martinsumner,0,,[],[],0,0,,f3aef17bb0204c1c77dc92c60cc27d6db45a6129,1628897,martinsumner,0,,develop-3.0,791522,basho/riak_kv,mas-i1824-crashfold,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
206,25970549425,PullRequestEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-19 10:51:53,1170560227,1841,Develop d30updated,Update from develop-3.0 (3.0.12 release),1628897,martinsumner,0,,[],[],0,0,,,0,,0,,develop,791522,basho/riak_kv,develop-d30updated,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
207,25971650588,PullRequestEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-19 11:44:58,1170560227,1841,,,1628897,martinsumner,0,,[],[],0,0,,d6357d89bd15286ed12b5f1e69ecbcfbafd9a601,1628897,martinsumner,0,,develop,791522,basho/riak_kv,develop-d30updated,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
208,26019211066,PullRequestEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-21 11:06:08,1173533401,1842,Use alternate OTP versions,,1628897,martinsumner,0,,[],[],0,0,,,0,,0,,develop,791522,basho/riak_kv,mas-gha-update25,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
209,26019660072,PullRequestEvent,closed,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-21 11:29:02,1173533401,1842,,,1628897,martinsumner,0,,[],[],0,0,,2713b89a7442f3a213e8ea4684af1d37b9d305b8,1628897,martinsumner,0,,develop,791522,basho/riak_kv,mas-gha-update25,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
210,26023948439,PullRequestEvent,opened,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-21 14:52:55,1173803685,1843,Update rebar.config,,1628897,martinsumner,0,,[],[],0,0,,,0,,0,,develop,791522,basho/riak_kv,develop-hyper_update,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
211,20168989321,WatchEvent,started,2155893,sekirocc,791522,basho/riak_kv,176293,basho,2022-02-10 08:39:13,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
212,21067748658,PullRequestReviewCommentEvent,created,1933698,s2hc-johan,791522,basho/riak_kv,176293,basho,2022-04-01 15:36:50,875789740,1816,,"> ... ona  subset of ...
should probably be
> ... on a subset of ...",1628897,martinsumner,0,,[],[],0,0,,99eb4c23495f397ebba893713771756653f16e8a,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1815-autocheck,929111223,840705400,docs/NextGenREPL.md,1933698,s2hc-johan,0,,[],[],[],0,0,,,,0,,,0,,,[]
213,20516979915,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-01 12:50:19,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9232868174,c3e7c64614ea604ca8367c00db9a05e8b99a5dd9,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Update riak_client.erl'],0,0,,,,0,,,0,,,[]
214,20522341116,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-01 17:37:59,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9235459970,0a90a2afcd0fa55589c534adccf285550582b508,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Update following test\n\nAdds some further logging.  Also corrects the comparison between current and discovered peers to avoid unnecessary resets.'],0,0,,,,0,,,0,,,[]
215,20539384114,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-02 11:21:51,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9243516682,746c6bb306b0461e26ddec2da64df27e0afa4465,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Updates following extension of test\n\nAdds operator riak_client functions'],0,0,,,,0,,,0,,,[]
216,20548285485,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-02 19:12:20,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9247870583,5656ce9a4fbe3ff31afc5738fc3c031255a8ddc3,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Close clients in work queue when terminating'],0,0,,,,0,,,0,,,[]
217,20562427130,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-03 13:18:01,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9255121296,23dd69f7c2e2e75dab9adcad7b50efbdb986fd6f,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Update riak_kv_replrtq_snk.erl'],0,0,,,,0,,,0,,,[]
218,20581267847,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-04 11:34:06,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9264556863,4c39146670fb2cab887985c300dca375b24d8e72,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Add configuration items'],0,0,,,,0,,,0,,,[]
219,20633885145,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-08 09:50:56,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9293262378,0d9b124ac1a793fcb968a65f41c79ec211599305,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Update riak_kv_replrtq_peer.erl'],0,0,,,,0,,,0,,,[]
220,20668430211,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-09 22:06:24,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9310308839,d1c7102129e70fc71f867cf72fdfc0a0db4a8a1b,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Add stats for ttaae full-sync'],0,0,,,,0,,,0,,,[]
221,20697634895,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-11 10:03:38,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9325049818,c92b74683e217adf821f38dcef46a211f0c25e36,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['add configuration for auto_check and all_check window'],0,0,,,,0,,,0,,,[]
222,20698058074,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-11 10:28:51,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9325257857,3ad7f3e7b58564c2833afec197c643b7643ef251,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Fix translation to use riak_kv as application'],0,0,,,,0,,,0,,,[]
223,20717302264,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-12 21:09:46,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9335641467,fee758661d5f4314d50f013a6a6777cf0db52431,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Add default dir'],0,0,,,,0,,,0,,,[]
224,20717462674,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-12 21:44:48,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9335753445,2096ac675f9e5f37a056078696aff46c7152e317,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['Whenever we add to overflow queue, must ensure local queue IsEmpty is false\n\nIf ObjectLimit == 0, then this might not be the case']",0,0,,,,0,,,0,,,[]
225,20739277963,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-14 16:13:18,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9347865385,692214f5c1fe7f89c5e82c614eef9bcf43243acc,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['Delete overflow queue when terminating\n\nThere will be no attempt to re-open overflow on startup.  The queue is persisted to avoid over-consumption of memory, not so that replication requests can be persisted across reboots']",0,0,,,,0,,,0,,,[]
226,20756350706,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-15 12:35:29,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9356405068,b5292aa7aadfb0c41604111e5405d5ecb17bbd54,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Change full-sync defaults to use autocheck and rangecheck'],0,0,,,,0,,,0,,,[]
227,20851609972,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-21 13:59:22,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9405997928,9e47706950910f0a95504a50a42f52468c8b89ac,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['clear_rtq\n\nAdd equivalent to reaper/eraser clear function, which can be used when resetting the size of overflow queues']",0,0,,,,0,,,0,,,[]
228,20853959646,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-21 15:42:55,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9407102443,7d88db9b138e1736b1030cc46a24f3fc4029a218,"['Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['clear_rtq\n\nAdd equivalent to reaper/eraser clear function, which can be used when resetting the size of overflow queues', ""Merge branch 'mas-i1817-overflowrtq' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
229,20870246862,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-22 11:14:17,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9415076730,db73fb67a3055c9739c21ac13b79ad9579464221,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['Allow overflow queues to be set in riak.conf\n\nDefault them all to the same value for simplicity, and name the configuration item consistently (although there is a subtle difference in the implementation for replrtq)']",0,0,,,,0,,,0,,,[]
230,20870745220,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-22 11:42:34,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9415318964,709e0bd9b824ec8332644bffbd2ad7e7386d2893,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Improve comments'],0,0,,,,0,,,0,,,[]
231,20871354588,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-22 12:16:47,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9415619430,33d00a9f07d9930d8d3a950a640e5516e0342e62,"['Martin Sumner', 'Martin Sumner', 'Martin Sumner', 'Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Allow overflow queues to be set in riak.conf\n\nDefault them all to the same value for simplicity, and name the configuration item consistently (although there is a subtle difference in the implementation for replrtq)', 'Improve comments', 'Reduce riak_kv_tombstone_pause - https://github.com/basho/riak_kv/issues/1819', ""Merge branch 'mas-i1817-overflowrtq' into mas-3.0.10-releasecandidate"", ""Merge branch 'mas-i1813-getfinalaction' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
232,20871662512,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-22 12:33:06,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9415770635,8e24fcbde59d6030b0f18f3ac7d4b6ec389cf917,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Correct merge'],0,0,,,,0,,,0,,,[]
233,20872108223,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-22 12:56:10,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9415991242,cb145437d5d1c1dcebf6c02cc561d0d44f59ed38,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Switch deps for https protocol fetch'],0,0,,,,0,,,0,,,[]
234,20890754843,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-23 10:34:30,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9425164008,11815964087c333921eb3c01d0eec03e45f3d4da,"['Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Updates following code review\n\nRename _discovery functions to reduce confusion over multiple things called prompt_discovery doing different things', 'Update to avoid https/git issue']",0,0,,,,0,,,0,,,[]
235,20891695390,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-23 11:25:17,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9425626605,44b556761f99d5bfeb2fc2a4d2d40a2b9009fb88,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Convert app_helper -> application in replrtq'],0,0,,,,0,,,0,,,[]
236,20900804022,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-23 19:09:59,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9430052530,60b19206310acb770cb68add9db7444a5484ef95,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Update rebar.config'],0,0,,,,0,,,0,,,[]
237,20913198388,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-24 11:08:24,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9436299509,98c25aba9df09bbdd1e58f9ffb7a954564ea443a,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Fix change in limit name'],0,0,,,,0,,,0,,,[]
238,20918854572,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-24 15:35:18,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9439116470,f4be9d55c1dc106ff4d0839030558fe2d53000b7,"['Martin Sumner', 'Martin Sumner', 'Martin Sumner', 'Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Updates following code review\n\nRename _discovery functions to reduce confusion over multiple things called prompt_discovery doing different things', 'Update to avoid https/git issue', 'Convert app_helper -> application in replrtq', 'app_helper:get/2 =/= application:get_env/2\n\nSaves some pain with pattern matching {ok, V}|undefined', ""Merge branch 'mas-i1804-peerdiscovery' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
239,20933921038,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-25 10:45:34,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9446691683,31dc8db5e5f54ad82b2496eb5fd97647afe82aa0,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Revert back as app_helper:get_env/2 =/= appplication:get_env/2'],0,0,,,,0,,,0,,,[]
240,20933932815,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-25 10:46:18,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9446697580,13536953d3dc5e71d7907f92fba4a71d46df137a,"['Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Revert back as app_helper:get_env/2 =/= appplication:get_env/2', ""Merge branch 'mas-i1804-peerdiscovery' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
241,20934105482,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-25 10:56:52,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9446783347,179a5432eece25975dbd7ec718193db94208dd89,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Update riak_kv_replrtq_peer.erl'],0,0,,,,0,,,0,,,[]
242,20934110865,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-25 10:57:12,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9446786016,3ed59af0e9bc260003756b8f14b29b9d63995b46,"['Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Update riak_kv_replrtq_peer.erl', ""Merge branch 'mas-i1804-peerdiscovery' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
243,20939368325,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-25 15:51:30,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9449361099,a50048bf4398ed21c41e47d1a9a57b5f6552685e,"['Martin Sumner', 'Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Fix change in limit name', 'Clear queue should reset size\n\nNeeds to change size limit stored on state.  Also remove previous (and now redundant) configuration: replrtq_srcqueuelimit now replaced with replrtq_overflow_limit', ""Merge branch 'mas-i1817-overflowrtq' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
244,21016698356,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-30 11:42:43,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9488985067,5839feda8c560704df9ddfa5aeed74d56eb1e963,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Documentation updates following review\n\nPreviously the documentation had stated that smart checks were only relevant to `bucket` or `type` scope.  In fact they can make the `all` scope more efficient in clock comparison.'],0,0,,,,0,,,0,,,[]
245,21016755461,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-30 11:45:53,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9489013668,fd2a5f73923549b53a0e24eddd26e2c2af714294,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Resolve git reference'],0,0,,,,0,,,0,,,[]
246,21028413312,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-30 22:06:07,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9494667216,9d0ad30e82bb22f96ce51c7addb2b426e88b4ced,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Add skip to auto_check\n\nRemoves need to split default config between range_check and auto_check'],0,0,,,,0,,,0,,,[]
247,21028950869,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-30 22:45:50,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9494922546,a0074eea1c43b3e18e6fbb010fb4b76b589b7bc4,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"[""Don't use previous success\n\nMay prompt range_check without range""]",0,0,,,,0,,,0,,,[]
248,21038543934,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-31 10:05:46,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9499516480,eb1aca012f009a1f6a6b8e4d0e8570324fda3c93,"['Martin Sumner', 'Martin Sumner', 'Martin Sumner', 'Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Documentation updates following review\n\nPreviously the documentation had stated that smart checks were only relevant to `bucket` or `type` scope.  In fact they can make the `all` scope more efficient in clock comparison.', 'Resolve git reference', 'Add skip to auto_check\n\nRemoves need to split default config between range_check and auto_check', ""Don't use previous success\n\nMay prompt range_check without range"", ""Merge branch 'mas-i1815-autocheck' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
249,21067346478,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-01 15:15:41,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9513486629,df87ee6ed0bea03b956c4166fe64b41dc2d4d744,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['Update auto-check\n\nfull-sync can now do bi-directional repairs!!\n\nThe auto_check is adapted further, so that there is no reason to split the default configuration between auto_check and range_check.  The auto_check method should almost always be the right one.']",0,0,,,,0,,,0,,,[]
250,21074135876,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-01 23:55:42,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9516861655,504be41c9d82e211adb0f6103380634d320e73ec,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['Correctly encode clocks pb/http\n\nThe encoding of vclocks differs between http and pb due to the need to base64 encode for http.\n\nCase clauses change for auto_check, as otherwise may have requested a range_check without a range or previous success, when drop_next_auto_check() was true, even when the peer queue is not diabled.']",0,0,,,,0,,,0,,,[]
251,21076436968,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-02 07:52:16,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9518302813,b73a60253bd79ab5084ec8e7b34ca5afd31a7bef,"['Martin Sumner', 'Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Update auto-check\n\nfull-sync can now do bi-directional repairs!!\n\nThe auto_check is adapted further, so that there is no reason to split the default configuration between auto_check and range_check.  The auto_check method should almost always be the right one.', 'Correctly encode clocks pb/http\n\nThe encoding of vclocks differs between http and pb due to the need to base64 encode for http.\n\nCase clauses change for auto_check, as otherwise may have requested a range_check without a range or previous success, when drop_next_auto_check() was true, even when the peer queue is not diabled.', ""Merge branch 'mas-i1815-autocheck' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
252,21140847099,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-06 11:10:28,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9552129272,2d4d1b9e32303db1f07223d229f25b33dd785883,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Use pb by default for replication\n\nAlso simplify the determine_next_action logic by making the target for making a range more obvious/predictable'],0,0,,,,0,,,0,,,[]
253,21147558984,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-06 16:38:42,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9555363539,dea67dbbdc594f4bdaf1bd8e21a5c8e2e9c4a3cf,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['Refinements after further volume testing\n\nThe coordination of syncs between clusters caused performance spikes in testing.  A simple (and optional) way round this is the configuration of cluster slice numbers.\n\nAlso, this change simplifies whether or not a range will be defined.\n\nAs auto_check used by default - other checks are just a ""finder"", and so can be accelerated by using less max_results.  Even with a relatively small sample size, significant ""clipping"" of the boundary is unlikely (e.g. only 3% chance the lmd range will be clipped by more than 10%).']",0,0,,,,0,,,0,,,[]
254,21147580131,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-06 16:39:53,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9555373422,d702cfe28d00debaad8cd254831f6e901920190e,"['Martin Sumner', 'Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Use pb by default for replication\n\nAlso simplify the determine_next_action logic by making the target for making a range more obvious/predictable', 'Refinements after further volume testing\n\nThe coordination of syncs between clusters caused performance spikes in testing.  A simple (and optional) way round this is the configuration of cluster slice numbers.\n\nAlso, this change simplifies whether or not a range will be defined.\n\nAs auto_check used by default - other checks are just a ""finder"", and so can be accelerated by using less max_results.  Even with a relatively small sample size, significant ""clipping"" of the boundary is unlikely (e.g. only 3% chance the lmd range will be clipped by more than 10%).', ""Merge branch 'mas-i1815-autocheck' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
255,21149639718,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-06 18:36:55,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9556368603,7e20ca6fe3e110a72b5e588ee757ccec7c0f9be5,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Doc update - new defaults'],0,0,,,,0,,,0,,,[]
256,21149657196,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-06 18:38:01,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9556377078,959027448ff226e86c751e47bac46c6fb32210f9,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Doc update - new defaults'],0,0,,,,0,,,0,,,[]
257,21161880454,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-07 10:05:15,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9562418532,89384f6bfcbab8c3d406b8fd27fce469847f60dc,"['Martin Sumner', 'Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Doc update - new defaults', 'Resolve schedule spacing\n\nSchedule spacing was dependent on start-up time being consistent, so now instead we use the beginning of the next hour as the anchor (following startup).', ""Merge branch 'mas-i1815-autocheck' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
258,21162715907,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-07 10:50:38,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9562833361,56663848e7662fc4f089784dc70273c1fb14bfe0,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Increase wait and info at startup\n\nInitial schedule time has not been as expected.  Increase logging'],0,0,,,,0,,,0,,,[]
259,21162720169,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-07 10:50:53,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9562835485,1975bf7fa261bbc3f4e0f0227c9b5feb3891609a,"['Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Increase wait and info at startup\n\nInitial schedule time has not been as expected.  Increase logging', ""Merge branch 'mas-i1815-autocheck' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
260,21162954887,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-07 11:04:06,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9562951964,e7ff1f7d87101dea223f424e2b830e0bd26dac28,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"[""Add resetting of schedule\n\nCluster could be in a change state at startup so relying on node counts etc at this point for next 24 hours seems too unstable.\n\nReset when ever node info changes, in case it wasn't right at first pass.""]",0,0,,,,0,,,0,,,[]
261,21162962962,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-07 11:04:34,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9562955996,e4b5dabc7d6747ed6347105ebeaa75ec186b2243,"['Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","[""Add resetting of schedule\n\nCluster could be in a change state at startup so relying on node counts etc at this point for next 24 hours seems too unstable.\n\nReset when ever node info changes, in case it wasn't right at first pass."", ""Merge branch 'mas-i1815-autocheck' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
262,21164097063,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-07 12:08:04,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9563514391,3978604a57b41fcbfec6287f0d2d02d939da7b74,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Remove redundant log'],0,0,,,,0,,,0,,,[]
263,21164102856,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-07 12:08:21,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9563517133,9b12a60a9daa2eb3cf2fea11e9854980663cc598,"['Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Remove redundant log', ""Merge branch 'mas-i1815-autocheck' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
264,21164583065,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-07 12:32:53,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9563757548,b3da19f2902d82523e2088d2ab7c94dee1b484ab,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"[""Assume node must eventually come up\n\nIn wild seen {4, 3, 1} returned when the node hasn't started after the initial timeout.  this node will always eventually be up, so always include this node in the UpNodes and avoid any situation where this node is n+1 of n.""]",0,0,,,,0,,,0,,,[]
265,21164586872,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-07 12:33:05,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9563759379,b207d4c35074f8c914363f14a5006cd473ca05b0,"['Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","[""Assume node must eventually come up\n\nIn wild seen {4, 3, 1} returned when the node hasn't started after the initial timeout.  this node will always eventually be up, so always include this node in the UpNodes and avoid any situation where this node is n+1 of n."", ""Merge branch 'mas-i1815-autocheck' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
266,21169584818,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-07 16:19:52,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9566133599,108d41cae1c9ea54eac2766c628f7317b74af0d0,"['Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Potential for broken trees\n\nIf the aae_trees are broken on multiple nodes - i.e. the same segment represented incorrectly in the same way on all 3 nodes, then internal anti-entropy cannot repair this.\n\nThis can lead to a situation where inter-cluster anti-entropy can never resolve.  This can be resolved by enforcing the rebuild of the trees within the aae_fold fetch clocks (although this has an efficiency overhead).\n\nRather than rely on operator intervention to trigger in this case, this code change will spot {clock_compare,0} on an all_check  and set this parameter on the node.  Eventually, if most nodes have peers, then this will be resolved a sit is turned on node by node.\n\nOnce a sync is successful, it will be turned back off.', ""Merge branch 'mas-i1815-autocheck' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
267,21279754110,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-13 23:31:09,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9621676745,e76174f24a9086c12d3e9e539c75b58863a15148,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Update rebar.config'],0,0,,,,0,,,0,,,[]
268,21358614675,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-19 16:26:34,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9663802653,f54ffa3c657087613b714b1d67e60d8df9c0a860,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"[""Catch object corruption in rebuild folds\n\nDon't add a corrupted object when rebuilding - instead log, skip past, and prompt read repair.\n\nThe behaviour before was to avoid crashing on inserting a corrupted object (whereby the vector clock cannot be extracted for hashing).  Not clear why this is still inserted.  This behaviour is maintained, with the alternative behaviour during the rebuild.\n\nThe same handle_corrupted_object/4 fun is then re-used during parallel mode tictac aae rebuilds, with a new version of kv_index_tictactree which catches exceptions and allows an external function to be used for handling them).""]",0,0,,,,0,,,0,,,[]
269,21358840195,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-19 16:39:07,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9663910367,ed7122b73c5e80dc81140a80bb39f178628efaa1,"['Martin Sumner', 'Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Add a reader queue\n\nUse the reader queue for repair_keys_range, but also to trigger a read repair when there is key amnesia on a non-coordinating node to resolve the inconsistency without waiting for AAE.', ""Catch object corruption in rebuild folds\n\nDon't add a corrupted object when rebuilding - instead log, skip past, and prompt read repair.\n\nThe behaviour before was to avoid crashing on inserting a corrupted object (whereby the vector clock cannot be extracted for hashing).  Not clear why this is still inserted.  This behaviour is maintained, with the alternative behaviour during the rebuild.\n\nThe same handle_corrupted_object/4 fun is then re-used during parallel mode tictac aae rebuilds, with a new version of kv_index_tictactree which catches exceptions and allows an external function to be used for handling them)."", ""Merge branch 'mas-i1813-keepamnesia' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
270,21359066811,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-19 16:52:10,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9664020522,e08bffd9f5f294564105a923ec02ddbd70375c80,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['monitorpool PR merged'],0,0,,,,0,,,0,,,[]
271,21363541969,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-19 21:38:06,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9666240718,20801786819c9926ae0eae033956f90a6e35478f,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['Pause/resume requires allocations to be clear\n\nOtherwise an allocation may have been prompted.  Now on timeout there will be no allocations, and a new set of no_checks.\n\nIn case a check was already in flight, we check pause on handle_info of work_item as well']",0,0,,,,0,,,0,,,[]
272,21363762886,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-19 21:56:01,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9666352256,a497cb9491de07ad090465258a35f8e9f9ac7793,"['Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Pause/resume requires allocations to be clear\n\nOtherwise an allocation may have been prompted.  Now on timeout there will be no allocations, and a new set of no_checks.\n\nIn case a check was already in flight, we check pause on handle_info of work_item as well', ""Merge branch 'mas-i1815-autocheck' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
273,21365096833,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-20 00:09:53,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9667054984,4e3d5fba6ad030dcefd0e15e6d1fe92f3d50e9c0,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Manage multiple loops'],0,0,,,,0,,,0,,,[]
274,21365099987,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-20 00:10:10,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9667056587,33e9cc41683c6636b41dc0ad3af27704b29f7abd,"['Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Manage multiple loops', ""Merge branch 'mas-i1815-autocheck' into mas-3.0.10-releasecandidate""]",0,0,,,,0,,,0,,,[]
275,21365409477,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-20 00:40:42,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9667212641,e76dd62e37d3a2eafef1a9b0a7f31bd361059e3a,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['Revert ""Manage multiple loops""\n\nThis reverts commit 4e3d5fba6ad030dcefd0e15e6d1fe92f3d50e9c0.']",0,0,,,,0,,,0,,,[]
276,21365412027,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-04-20 00:40:57,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9667213846,baa7f8219a62682fb362e8ba200948b51c77f86d,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['Revert ""Merge branch \'mas-i1815-autocheck\' into mas-3.0.10-releasecandidate""\n\nThis reverts commit 33e9cc41683c6636b41dc0ad3af27704b29f7abd, reversing\nchanges made to a497cb9491de07ad090465258a35f8e9f9ac7793.']",0,0,,,,0,,,0,,,[]
277,25592021825,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-01 13:41:02,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,11848563843,7da011261e47bddfc6ec0458aad5430ed73e40b5,['Andrei Zavada'],['johnhommer@gmail.com'],"['add missing function clause repair_keys_range in convert_fold (#1839)\n\n* add missing function clause repair_keys_range in convert_fold, to unbreak aae_fold for that case\r\n\r\n* thread converted aae_fold query in riak_client, to complete prev commit']",0,0,,,,0,,,0,,,[]
278,25592031550,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-01 13:41:26,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,11848568758,bb5b610a0de865886312528893c4b990bc1517fd,['Peter Tihanyi'],['tihi@systream.hu'],['Change the default PR value from 0 to default (#1836)\n\nThis means that get requests will use bucket-type level Primary Read settings\r\n\r\nCo-authored-by: Peter Tihanyi <peter.tihanyi@otpbank.hu>'],0,0,,,,0,,,0,,,[]
279,25693452477,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-06 15:12:28,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,11901324451,adddf491c1b5a6839ea6be6d19756a2d5d17574b,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Add warning to update riak.conf file after reip'],0,0,,,,0,,,0,,,[]
280,25694397028,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-06 15:44:45,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,11901761075,b2b5fd3f135b3fce49ad4f6a001b8209c75f33f2,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"[""Make clear where attention is required\n\nAnd return 'ok' to make clear the op was successful""]",0,0,,,,0,,,0,,,[]
281,25698097273,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-06 18:00:56,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,11903462664,159df407282e67b57cb7a43f36ed375324fcb035,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"[""Mas i1121 reip3 (#1838)\n\n* Add reip/3\r\n\r\nTo allow for reip without loading the riak_core application\r\n\r\n* Use alternate name\r\n\r\n* Update riak_kv_console.erl\r\n\r\n* Update riak_kv_console.erl\r\n\r\n* reip_manual inputs are atoms\r\n\r\n* Add warning to update riak.conf file after reip\r\n\r\n* Make clear where attention is required\r\n\r\nAnd return 'ok' to make clear the op was successful\r\n\r\n* Update rebar.config""]",0,0,,,,0,,,0,,,[]
282,25925946048,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-16 04:28:44,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,12016195250,f3aef17bb0204c1c77dc92c60cc27d6db45a6129,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['Update log (#1840)\n\nMake situation clearer in log\r\n\r\nRather than change behaviour, just make it clear that the warning can be ignored when shutting down.  This should avoid unnecessary concern.']",0,0,,,,0,,,0,,,[]
283,25938431790,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-16 16:04:02,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,12022426013,4ddb2aa80959b07061b3d367031fde0f8d4c344e,"['Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Branch switch back to d-3.0', ""Merge branch 'develop-3.0' of https://github.com/basho/riak_kv into develop-3.0""]",0,0,,,,0,,,0,,,[]
284,25970056028,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-19 10:27:48,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,12041134675,1e19c3238967b8e4784ebc35e44027654399e411,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Update to tags for release'],0,0,,,,0,,,0,,,[]
285,25971376083,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-19 11:30:20,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,12041771918,e1d7bbcc3707da097ae40e37aee18aa6c5a1dbf3,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['Update Readme, remove mercurial refs']",0,0,,,,0,,,0,,,[]
286,25971650772,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-19 11:44:59,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,12041908054,d6357d89bd15286ed12b5f1e69ecbcfbafd9a601,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"[""Develop d30updated (#1841)\n\n* Do not crash when object's contents is an empty list (#1837)\r\n\r\n* Add test to illustrate issue\r\n\r\n* Do not crash when object's contents is an empty list\r\n\r\n* add missing function clause repair_keys_range in convert_fold (#1839)\r\n\r\n* add missing function clause repair_keys_range in convert_fold, to unbreak aae_fold for that case\r\n\r\n* thread converted aae_fold query in riak_client, to complete prev commit\r\n\r\n* Change the default PR value from 0 to default (#1836)\r\n\r\nThis means that get requests will use bucket-type level Primary Read settings\r\n\r\nCo-authored-by: Peter Tihanyi <peter.tihanyi@otpbank.hu>\r\n\r\n* Mas i1121 reip3 (#1838)\r\n\r\n* Add reip/3\r\n\r\nTo allow for reip without loading the riak_core application\r\n\r\n* Use alternate name\r\n\r\n* Update riak_kv_console.erl\r\n\r\n* Update riak_kv_console.erl\r\n\r\n* reip_manual inputs are atoms\r\n\r\n* Add warning to update riak.conf file after reip\r\n\r\n* Make clear where attention is required\r\n\r\nAnd return 'ok' to make clear the op was successful\r\n\r\n* Update rebar.config\r\n\r\n* Update log (#1840)\r\n\r\nMake situation clearer in log\r\n\r\nRather than change behaviour, just make it clear that the warning can be ignored when shutting down.  This should avoid unnecessary concern.\r\n\r\n* Update Readme, remove mercurial refs\r\n\r\nCo-authored-by: Enrique Fernández <efcasado@gmail.com>\r\nCo-authored-by: Andrei Zavada <johnhommer@gmail.com>\r\nCo-authored-by: Peter Tihanyi <tihi@systream.hu>\r\nCo-authored-by: Peter Tihanyi <peter.tihanyi@otpbank.hu>""]",0,0,,,,0,,,0,,,[]
287,25972418315,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-19 12:21:01,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,12042283116,6442797f0ed667016ba8f7e8ddd4664b51c3316a,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Switch README to markdown'],0,0,,,,0,,,0,,,[]
288,26019265621,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-21 11:08:48,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,12065856195,eec186019e01788a2530986d88ec528f0d709bee,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Use 25.1'],0,0,,,,0,,,0,,,[]
289,26019660156,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-21 11:29:02,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,12066050928,2713b89a7442f3a213e8ea4684af1d37b9d305b8,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Use alternate OTP versions (#1842)\n\n* Use alternate OTP versions\r\n\r\n* Use 25.1'],0,0,,,,0,,,0,,,[]
290,20130288269,PullRequestReviewEvent,created,3169010,martincox,791522,basho/riak_kv,176293,basho,2022-02-08 13:18:57,842746567,1810,,,1628897,martinsumner,0,,[],[],0,0,,7b4a76601bd3caead83f36e4134aa5c727149cff,0,,0,,develop,791522,basho/riak_kv,develop-eqc-otp24,876012420,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
291,20152844361,PullRequestReviewEvent,created,3169010,martincox,791522,basho/riak_kv,176293,basho,2022-02-09 14:02:28,842900838,1811,,,1628897,martinsumner,0,,[],[],0,0,,b0a5848e7a91f92b62760a410ab38bb7daa27267,0,,0,,develop,791522,basho/riak_kv,mas-mrc-eunitfix,877503947,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
292,25400300746,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-11-22 11:34:26,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,11747952203,7ed0b98d3a87a4d3888e470300e33e3dcd8b537f,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Use alternate name'],0,0,,,,0,,,0,,,[]
293,25404885254,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-11-22 14:39:26,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,11750209908,09f3d256212546667e1902912c7803b3e3fd8f89,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Update riak_kv_console.erl'],0,0,,,,0,,,0,,,[]
294,25408757199,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-11-22 17:09:24,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,11752060764,a8addf524b7dd4c6bb5d06ea7ac8ba86214e0220,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['reip_manual inputs are atoms'],0,0,,,,0,,,0,,,[]
295,25428457482,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-11-23 12:39:30,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,11762103743,9ca718423ea2f37a653b933886ef1650a5e02de7,['Enrique Fernández'],['efcasado@gmail.com'],"[""Do not crash when object's contents is an empty list (#1837)\n\n* Add test to illustrate issue\r\n\r\n* Do not crash when object's contents is an empty list""]",0,0,,,,0,,,0,,,[]
296,21728886075,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-11 10:07:38,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9852094578,2979109afef7c0b12bcb457210b7ab0d7e677236,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"[""Mas i1807 overflowqueues (#1809)\n\n* Bound reap/erase queues\r\n\r\nAlso don't log queues on a crash - avoid over-sized crash dumps\r\n\r\n* Create generic behaviour for eraser/reaper\r\n\r\nHave the queue backed to disk, so that beyond a certain size it overflows from memory to disk (and once the on disk part is consumed from the queue the files are removed).\r\n\r\n* Stop cleaning folder\r\n\r\nRisk of misconfiguration leading to wiping of wrong data.  Also starting a job may lead to the main process having its disk_log wiped.\r\n\r\n* Setup folders correctly\r\n\r\nAvoid enoent errors\r\n\r\n* Correct log\r\n\r\n* Further log correction\r\n\r\n* Correct cleaning of directories for test\r\n\r\n* Switch to action/2\r\n\r\n* Update eqc tests for refactoring of reaper/eraser\r\n\r\n* Improve comments/API\r\n\r\n* Pause reaper on overload\r\n\r\nCheck for a soft overload on any vnode before reaping.  This will add some delay - but reap has been show to potentially overload a cluster ... availability is more important than reap speed.\r\n\r\nThere is no catch for {error, mailbox_overload} should it occur - it should not as the mailbox check should prevent it.  If it does, the reaper will crash (and restart without any reaps) - return to a safe known position.\r\n\r\n* Adjustments following review\r\n\r\nThe queue file generated, are still in UUID format, but the id now incorporates creation date.  This should make it easier to detect and clean any garbage that might be accrued.\r\n\r\nOne use case where '++' is used has also been removed (although a lists:flatten/1 was still required at the end in this case)""]",0,0,,,,0,,,0,,,[]
297,21729112066,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-11 10:19:11,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9852204297,9e3abfde6ac17617327a327a89fdb2c48376fbe6,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Switch to develop-3.0'],0,0,,,,0,,,0,,,[]
298,21729551484,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-11 10:42:14,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9852412831,49dbe503b47e7a9200b15da9496098a91fea7557,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Update rebar.config'],0,0,,,,0,,,0,,,[]
299,21732836330,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-11 13:27:08,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9853994458,aeca1cab68de678909fe4a81ec9456ca4d93a1a4,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['Mas i1804 peerdiscovery (#1812)\n\nSee #1804\r\n\r\nThe heart of the problem is how to avoid needing configuration changes on sink clusters when source clusters are bing changed. This allows for new nodes to be discovered automatically, from configured nodes.\r\n\r\nDefault behaviour is to always fallback to configured behaviour.\r\n\r\nWorker Counts and Per Peer Limits need to be set based on an understanding of whether this will be enabled. Although, if per peer limit is left to default, the consequence will be the worker count will be evenly distributed (independently by each node). Note, if Worker Count mod (Src Node Count) =/= 0 - then there will be no balancing of the excess workers across the sink nodes.']",0,0,,,,0,,,0,,,[]
300,21755892277,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 14:12:51,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9865391178,5f63a4aaed8219c7b258ddcee73a7b853b78b07c,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Update rebar.config'],0,0,,,,0,,,0,,,[]
301,21756882546,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 14:55:39,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9865861072,04376ff497774f0811713d127b69b83738d1e83b,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['Allow for a dual final action - repair and delete (#1814)\n\n* Allow for a dual final action - repair and delete\r\n\r\nIf there is repair only (as a priority over delete - which really means trigger reap if required), then thee are scenarios where tombstones can perpetually re-appear, especially were there is key amnesia.\r\n\r\n* Reduce riak_kv_tombstone_pause - https://github.com/basho/riak_kv/issues/1819\r\n\r\n* Switch deps for https protocol fetch']",0,0,,,,0,,,0,,,[]
302,21757817459,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 15:38:47,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9866311979,33547a41e7a4e6738f3d4dd2832f2b0d738a3bbf,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['Mas i1815 autocheck (#1816)\n\nTo simplify the configuration, rather than have the operator select all_check day_check range_check etc, there is now a default strategy of auto_check which tries to do a sensible thing:\r\n\r\ndo range_check when a range is set, otherwise\r\ndo all_check if it is out of hours (in the all_check window), otherwise\r\ndo day_check\r\nSome stats added to help with monitoring, the detail is still also in the console logs.\r\n\r\nSee #1815']",0,0,,,,0,,,0,,,[]
303,21758698949,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 16:24:13,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9866736985,d2067f40858ba8a920aad93cf88c202e371f557c,"['Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Mas i1815 autocheck (#1816)\n\nTo simplify the configuration, rather than have the operator select all_check day_check range_check etc, there is now a default strategy of auto_check which tries to do a sensible thing:\r\n\r\ndo range_check when a range is set, otherwise\r\ndo all_check if it is out of hours (in the all_check window), otherwise\r\ndo day_check\r\nSome stats added to help with monitoring, the detail is still also in the console logs.\r\n\r\nSee #1815', ""Merge branch 'develop-3.0' into mas-i1817-overflowrtq""]",0,0,,,,0,,,0,,,[]
304,21759973532,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 17:39:09,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9867358583,3d2d4e232000b0a48254788fe0aadad25e4f547a,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"[""Mas i1817 overflowrtq (#1829)\n\nExpand on use of riak_kv_overflow_queue so that it is used by the riak_kv_replrtq_src, as well as riak_kv_reaper and riak_kv_eraser.\r\n\r\nThis means that larger queue sizes can be supported for riak_kv_replrtq_src without having to worry about compromising the memory of the node. This should allow for repl_keys_range AAE folds to generate very large replication sets, without clogging the node worker pool by pausing so that real-time replication can keep up.\r\n\r\nThe overflow queues are deleted on shutdown (if there is a queue on disk). The feature is to allow for larger queues without memory exhaustion, persistence is not used to persist queues across restarts.\r\n\r\nOverflow Queues extended to include a 'reader' queue which may be used for read_repairs. Currently this queue is only used for the repair_keys_range query and the read-repair trigger.""]",0,0,,,,0,,,0,,,[]
305,21760731592,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 18:26:36,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9867732763,b0a633f76d6a6a4b653efc34498dc03d08824726,"['Martin Sumner', 'Martin Sumner', 'Martin Sumner', 'Martin Sumner', 'Martin Sumner', 'Martin Sumner', 'Martin Sumner', 'Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","[""Mas i1807 overflowqueues (#1809)\n\n* Bound reap/erase queues\r\n\r\nAlso don't log queues on a crash - avoid over-sized crash dumps\r\n\r\n* Create generic behaviour for eraser/reaper\r\n\r\nHave the queue backed to disk, so that beyond a certain size it overflows from memory to disk (and once the on disk part is consumed from the queue the files are removed).\r\n\r\n* Stop cleaning folder\r\n\r\nRisk of misconfiguration leading to wiping of wrong data.  Also starting a job may lead to the main process having its disk_log wiped.\r\n\r\n* Setup folders correctly\r\n\r\nAvoid enoent errors\r\n\r\n* Correct log\r\n\r\n* Further log correction\r\n\r\n* Correct cleaning of directories for test\r\n\r\n* Switch to action/2\r\n\r\n* Update eqc tests for refactoring of reaper/eraser\r\n\r\n* Improve comments/API\r\n\r\n* Pause reaper on overload\r\n\r\nCheck for a soft overload on any vnode before reaping.  This will add some delay - but reap has been show to potentially overload a cluster ... availability is more important than reap speed.\r\n\r\nThere is no catch for {error, mailbox_overload} should it occur - it should not as the mailbox check should prevent it.  If it does, the reaper will crash (and restart without any reaps) - return to a safe known position.\r\n\r\n* Adjustments following review\r\n\r\nThe queue file generated, are still in UUID format, but the id now incorporates creation date.  This should make it easier to detect and clean any garbage that might be accrued.\r\n\r\nOne use case where '++' is used has also been removed (although a lists:flatten/1 was still required at the end in this case)"", 'Switch to develop-3.0', 'Update rebar.config', 'Mas i1804 peerdiscovery (#1812)\n\nSee #1804\r\n\r\nThe heart of the problem is how to avoid needing configuration changes on sink clusters when source clusters are bing changed. This allows for new nodes to be discovered automatically, from configured nodes.\r\n\r\nDefault behaviour is to always fallback to configured behaviour.\r\n\r\nWorker Counts and Per Peer Limits need to be set based on an understanding of whether this will be enabled. Although, if per peer limit is left to default, the consequence will be the worker count will be evenly distributed (independently by each node). Note, if Worker Count mod (Src Node Count) =/= 0 - then there will be no balancing of the excess workers across the sink nodes.', 'Update rebar.config', 'Allow for a dual final action - repair and delete (#1814)\n\n* Allow for a dual final action - repair and delete\r\n\r\nIf there is repair only (as a priority over delete - which really means trigger reap if required), then thee are scenarios where tombstones can perpetually re-appear, especially were there is key amnesia.\r\n\r\n* Reduce riak_kv_tombstone_pause - https://github.com/basho/riak_kv/issues/1819\r\n\r\n* Switch deps for https protocol fetch', 'Mas i1815 autocheck (#1816)\n\nTo simplify the configuration, rather than have the operator select all_check day_check range_check etc, there is now a default strategy of auto_check which tries to do a sensible thing:\r\n\r\ndo range_check when a range is set, otherwise\r\ndo all_check if it is out of hours (in the all_check window), otherwise\r\ndo day_check\r\nSome stats added to help with monitoring, the detail is still also in the console logs.\r\n\r\nSee #1815', ""Mas i1817 overflowrtq (#1829)\n\nExpand on use of riak_kv_overflow_queue so that it is used by the riak_kv_replrtq_src, as well as riak_kv_reaper and riak_kv_eraser.\r\n\r\nThis means that larger queue sizes can be supported for riak_kv_replrtq_src without having to worry about compromising the memory of the node. This should allow for repl_keys_range AAE folds to generate very large replication sets, without clogging the node worker pool by pausing so that real-time replication can keep up.\r\n\r\nThe overflow queues are deleted on shutdown (if there is a queue on disk). The feature is to allow for larger queues without memory exhaustion, persistence is not used to persist queues across restarts.\r\n\r\nOverflow Queues extended to include a 'reader' queue which may be used for read_repairs. Currently this queue is only used for the repair_keys_range query and the read-repair trigger."", ""Merge branch 'develop-3.0' into mas-i1813-keepamnesia""]",0,0,,,,0,,,0,,,[]
306,21761922282,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 19:42:56,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9868322211,0a1b69d335b4fe69a30a8f56bb17b18eaa51ee2f,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Fix typos due to c&p errors'],0,0,,,,0,,,0,,,[]
307,21774958757,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-13 12:58:29,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9874969737,71923c0dfc10cadfc45e0e31e2131201426338da,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['Mas i1813 keepamnesia (#1830)\n\nIntroduces a reader overflowq for doing read repair operations.  Initially this is used for:\r\n\r\n- repair_keys_range aae_fold - avoids the pausing of the fold that would block the worker pool;\r\n- repair on key_amnesia - triggers the required repair rather than causing an AAE delta;\r\n- repair on finding a corrupted object when folding to rebuild aae_store - previously the fold would crash, and the AAE store would therefore never be rebuilt.  [This PR](https://github.com/martinsumner/kv_index_tictactree/pull/106) is required to make this consistent in both AAE solutions.']",0,0,,,,0,,,0,,,[]
308,21775520998,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-13 13:28:05,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9875245709,0a5f60faeb11a59ce15acf249c9464f58853c609,"['Martin Sumner', 'Martin Sumner']","['martin.sumner@adaptip.co.uk', 'martin.sumner@adaptip.co.uk']","['Merge corrections\n\nMerges removed the stat updates for ttaae full-sync (detected by riak_test).\n\nA log had been introduced in riak_kv_replrtq_peer what could crash (detected by riak_test).\n\nThe safety change to avoid coordination in full-sync by setting time for first work item from beginning of next hour, makes sense with 24 slices (one per hour) ... but less sense with different values.  riak_test which uses a very high slice_count to avoid delays then failed.', 'Copy/Paste comment error']",0,0,,,,0,,,0,,,[]
309,21807380650,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-16 11:39:18,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,9893222784,c28826b2f22f6bf35294104898251c4d3743c6e8,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],"['Schema backwards compatability\n\nThe replrtq_srcqueuelimit was removed, as the queue is now an overflow queue and should have its limit configured via replrtq_overflow_limit.\n\nHowever, removing the old config item altogether will result in upgrades not being possible without changes to configuration files - so the old option is instead retained as a hidden config item to be ignored.']",0,0,,,,0,,,0,,,[]
310,22043542221,PushEvent,added,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-29 18:20:32,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,10013302840,c656fb47f3933d1c0d60f3469f7b0158d0c39e54,['Martin Sumner'],['martin.sumner@adaptip.co.uk'],['Tags for release'],0,0,,,,0,,,0,,,[]
311,25970973718,ReleaseEvent,published,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-19 11:11:24,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,86517037,riak_kv-3.0.12,Riak KV 3.0.12 - Release,1628897,martinsumner,"A number of minor fixes, along with leveled improvements.",[]
312,22043847025,ReleaseEvent,published,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-29 19:17:33,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,68100813,riak_kv-3.0.10,Riak KV 3.0.10 - Release,1628897,martinsumner,"Changes in two broad areas:
- Improvements to replication: peer discovery, auto_check in full-sync.
- Add an overflow queue feature, that will write to disk to avoid overflowing the memory, and applied to the following workers: reaper, eraser, replrtq_src and reader (a new worker for queueing read repairs).",[]
313,21204271081,WatchEvent,started,5256711,sleepiejohn,791522,basho/riak_kv,176293,basho,2022-04-09 23:44:51,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
314,21512705901,WatchEvent,started,362587,hengestone,791522,basho/riak_kv,176293,basho,2022-04-28 06:57:42,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
315,20521415227,WatchEvent,started,12005031,Krien,791522,basho/riak_kv,176293,basho,2022-03-01 16:45:43,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
316,20574793473,WatchEvent,started,46670806,noneback,791522,basho/riak_kv,176293,basho,2022-03-04 02:36:01,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
317,20695302639,WatchEvent,started,1087071,lihuibng,791522,basho/riak_kv,176293,basho,2022-03-11 07:29:37,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
318,20818017897,WatchEvent,started,28741910,rigzba21,791522,basho/riak_kv,176293,basho,2022-03-18 12:36:39,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
319,20828595566,WatchEvent,started,4591368,jtuki,791522,basho/riak_kv,176293,basho,2022-03-19 03:30:16,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
320,20845933072,WatchEvent,started,681982,hlmerscher,791522,basho/riak_kv,176293,basho,2022-03-21 08:50:43,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
321,25187070317,WatchEvent,started,48396023,Bnowako,791522,basho/riak_kv,176293,basho,2022-11-11 19:10:32,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
322,25643443724,WatchEvent,started,109225504,xyztony,791522,basho/riak_kv,176293,basho,2022-12-04 15:30:08,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
323,25664352228,WatchEvent,started,56402156,fkrause98,791522,basho/riak_kv,176293,basho,2022-12-05 14:59:42,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
324,25792224246,WatchEvent,started,19208687,MuhtasimTanmoy,791522,basho/riak_kv,176293,basho,2022-12-10 07:08:21,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
325,25910830526,WatchEvent,started,10611324,VonAlex,791522,basho/riak_kv,176293,basho,2022-12-15 13:52:10,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
326,21576063838,WatchEvent,started,46179734,mahesh-hegde,791522,basho/riak_kv,176293,basho,2022-05-02 13:52:21,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
327,22057233089,WatchEvent,started,36025028,hritvikpatel4,791522,basho/riak_kv,176293,basho,2022-05-30 13:56:10,0,0,,,0,,0,,[],[],0,0,,,0,,0,,,0,,,0,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
328,21067748483,PullRequestReviewEvent,created,1933698,s2hc-johan,791522,basho/riak_kv,176293,basho,2022-04-01 15:36:51,875789740,1816,,,1628897,martinsumner,0,,[],[],0,0,,99eb4c23495f397ebba893713771756653f16e8a,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1815-autocheck,929111223,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
329,20888695409,PullRequestReviewEvent,created,638015,ThomasArts,791522,basho/riak_kv,176293,basho,2022-03-23 08:46:41,870501164,1812,,,1628897,martinsumner,0,,[],[],0,0,,ae6acae02211d22deb3a42407322df600f98364e,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918398367,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
330,20888787114,PullRequestReviewEvent,created,638015,ThomasArts,791522,basho/riak_kv,176293,basho,2022-03-23 08:51:51,870501164,1812,,,1628897,martinsumner,0,,[],[],0,0,,ae6acae02211d22deb3a42407322df600f98364e,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918404917,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
331,20888830251,PullRequestReviewEvent,created,638015,ThomasArts,791522,basho/riak_kv,176293,basho,2022-03-23 08:54:14,870501164,1812,,,1628897,martinsumner,0,,[],[],0,0,,ae6acae02211d22deb3a42407322df600f98364e,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918407923,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
332,20889228686,PullRequestReviewEvent,created,638015,ThomasArts,791522,basho/riak_kv,176293,basho,2022-03-23 09:15:37,870501164,1812,,,1628897,martinsumner,0,,[],[],0,0,,ae6acae02211d22deb3a42407322df600f98364e,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918436366,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
333,20889377847,PullRequestReviewEvent,created,638015,ThomasArts,791522,basho/riak_kv,176293,basho,2022-03-23 09:23:34,870501164,1812,,,1628897,martinsumner,0,,[],[],0,0,,ae6acae02211d22deb3a42407322df600f98364e,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918446834,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
334,20889472410,PullRequestReviewEvent,created,638015,ThomasArts,791522,basho/riak_kv,176293,basho,2022-03-23 09:28:24,870501164,1812,,,1628897,martinsumner,0,,[],[],0,0,,ae6acae02211d22deb3a42407322df600f98364e,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918453410,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
335,20889502954,PullRequestReviewEvent,created,638015,ThomasArts,791522,basho/riak_kv,176293,basho,2022-03-23 09:29:54,870501164,1812,,,1628897,martinsumner,0,,[],[],0,0,,ae6acae02211d22deb3a42407322df600f98364e,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918455530,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
336,20889678920,PullRequestReviewEvent,created,638015,ThomasArts,791522,basho/riak_kv,176293,basho,2022-03-23 09:39:03,870501164,1812,,"Some smaller comments, nothing structural, just making sure you did things on purpose.
",1628897,martinsumner,0,,[],[],0,0,,ae6acae02211d22deb3a42407322df600f98364e,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918468714,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
337,20890047295,PullRequestReviewEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-23 09:58:05,870501164,1812,,,1628897,martinsumner,0,,[],[],0,0,,ae6acae02211d22deb3a42407322df600f98364e,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918497595,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
338,20891631855,PullRequestReviewEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-23 11:21:38,870501164,1812,,,1628897,martinsumner,0,,[],[],0,0,,ecd3ea830f6a3352fa2e7a9e9ecd3a2c53b2928f,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1804-peerdiscovery,918605094,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
339,20891655797,PullRequestReviewEvent,created,1933698,s2hc-johan,791522,basho/riak_kv,176293,basho,2022-03-23 11:23:01,875789740,1816,,"Assuming L249-L261 in `docs/NextGenREPL.md` describes defaults there are inconsistencies with what is defined in the `riak_kv.schema`.

Also the default window of 0-23 if window is configured isn’t in the documentation either I think.

As a summary of the PR I don’t see any code change; but added docs, configuration and tests.",1628897,martinsumner,0,,[],[],0,0,,67adba8bd2def59b0699775c496d34041029bb63,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1815-autocheck,918568275,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
340,20892146178,PullRequestReviewEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-23 11:50:50,875789740,1816,,,1628897,martinsumner,0,,[],[],0,0,,67adba8bd2def59b0699775c496d34041029bb63,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1815-autocheck,918638610,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
341,21016095228,PullRequestReviewEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-03-30 11:09:24,875789740,1816,,,1628897,martinsumner,0,,[],[],0,0,,67adba8bd2def59b0699775c496d34041029bb63,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1815-autocheck,925940759,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
342,25591999166,PullRequestReviewEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-12-01 13:40:05,1139726647,1839,,,1107079,hmmr,0,,[],[],0,0,,13df7b6ea74148915357c0f8a29f8d47018335eb,0,,0,,develop-3.0,378946268,hmmr/riak_kv,patch-1,1201006675,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
343,25457521434,PullRequestReviewEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-11-24 15:22:03,1117355782,1836,,,1583029,systream,0,,[],[],0,0,,158b48e5ae5f19f7571d6119896b99f4c5058d84,0,,0,,develop-3.0,424169838,systream/riak_kv,develop-3.0,1193328855,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
344,21759637744,PullRequestReviewEvent,created,1628897,martinsumner,791522,basho/riak_kv,176293,basho,2022-05-12 17:18:45,935027731,1829,,,1628897,martinsumner,0,,[],[],0,0,,dbd20f3316c966a2a1568a2c61fd7c90f5195f04,0,,0,,develop-3.0,791522,basho/riak_kv,mas-i1817-overflowrtq,971212584,0,,0,,0,,[],[],[],0,0,,,,0,,,0,,,[]
